{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io\n",
    "from skimage.measure import label, regionprops\n",
    "import glob\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class ImageAnalysis:\n",
    "    \n",
    "    \"\"\"\n",
    "    Provides methods for organizing and analyzing image data from various directory structures within a project folder.\n",
    "    This class initializes by reading the project folder, creating a DataFrame that lists each directory and its path,\n",
    "    and allows for further expansion to include specific image analysis metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - project_folder (str): The path to the project folder containing various image data directories.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_folder):\n",
    "        self.project_folder = project_folder\n",
    "        self.directory_df = self.initialize_directory_df() \n",
    "        \n",
    "    def initialize_directory_df(self):\n",
    "        directories = [d for d in os.listdir(self.project_folder) if os.path.isdir(os.path.join(self.project_folder, d))]\n",
    "        directory_data = [{'directory_name': d, 'directory_path': os.path.join(self.project_folder, d)} for d in directories]\n",
    "        return pd.DataFrame(directory_data, columns=['directory_name', 'directory_path'])\n",
    "    \n",
    "    def expand_directory_df(self):\n",
    "        # Add new columns with default empty lists\n",
    "        self.directory_df['sensor_type'] = ''\n",
    "        self.directory_df['session_id'] = ''\n",
    "        self.directory_df['stimulation_ids'] = [[] for _ in range(len(self.directory_df))]\n",
    "        self.directory_df['stimulation_frame_number'] = [[] for _ in range(len(self.directory_df))]\n",
    "\n",
    "        for index, row in self.directory_df.iterrows():\n",
    "            folder_name = row['directory_name']\n",
    "            folder_path = row['directory_path']\n",
    "            \n",
    "            # Parse folder name for sensor type and session id\n",
    "            parts = folder_name.split('_')\n",
    "            sensor_type = 'gcamp8' if parts[0].startswith('g') else 'cablam'\n",
    "            session_id = parts[0][1:] + parts[1]  # Assuming the first part is always the experiment ID\n",
    "\n",
    "            # Update DataFrame with sensor_type and session_id\n",
    "            self.directory_df.at[index, 'sensor_type'] = sensor_type\n",
    "            self.directory_df.at[index, 'session_id'] = session_id\n",
    "\n",
    "            # Check for CSV file ending in 'biolumi' or 'fluor'\n",
    "            csv_filename = [f for f in os.listdir(folder_path) if (f.endswith('biolumi.csv') or f.endswith('fluor.csv'))]\n",
    "            if csv_filename:\n",
    "                csv_file_path = os.path.join(folder_path, csv_filename[0])\n",
    "                df_csv = pd.read_csv(csv_file_path, header=None)\n",
    "                stimulation_ids = df_csv.iloc[1].dropna().tolist()\n",
    "                stimulation_frame_number = df_csv.iloc[0].dropna().tolist()\n",
    "\n",
    "                # Update DataFrame with stimulation information\n",
    "                self.directory_df.at[index, 'stimulation_ids'] = stimulation_ids\n",
    "                self.directory_df.at[index, 'stimulation_frame_number'] = stimulation_frame_number\n",
    "\n",
    "        return self.directory_df\n",
    "        \n",
    "    def max_projection_mean_values(self, tif_path):\n",
    "        \"\"\"\n",
    "        Generates a maximum intensity projection based on the mean values of a multi-frame TIF file\n",
    "        and saves it to a new subdirectory 'processed_data/processed_image_analysis_output'\n",
    "        with a '_max_projection' suffix in the file name.\n",
    "\n",
    "        Parameters:\n",
    "        tif_path (str): Path to the multi-frame TIF file.\n",
    "\n",
    "        Returns:\n",
    "        str: Path to the saved maximum intensity projection image.\n",
    "        \"\"\"\n",
    "\n",
    "        with Image.open(tif_path) as img:\n",
    "            # Initialize a summing array with the shape of the first frame and float type for mean calculation\n",
    "            sum_image = np.zeros((img.height, img.width), dtype=np.float32)\n",
    "\n",
    "            # Sum up all frames\n",
    "            for i in range(img.n_frames):\n",
    "                img.seek(i)\n",
    "                sum_image += np.array(img, dtype=np.float32)\n",
    "\n",
    "            # Compute the mean image by dividing the sum by the number of frames\n",
    "            mean_image = sum_image / img.n_frames\n",
    "        \n",
    "        # Define the new directory path\n",
    "        processed_dir = os.path.join(os.path.dirname(tif_path), 'processed_data', 'processed_image_analysis_output')\n",
    "        \n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(processed_dir, exist_ok=True)\n",
    "        \n",
    "        # Create a new file path for the max projection image with the '_max_projection' suffix\n",
    "        # The filename is extracted from tif_path and appended with '_max_projection.tif'\n",
    "        file_name = os.path.basename(tif_path)\n",
    "        max_proj_image_path = os.path.join(processed_dir, file_name.replace('.tif', '_max_projection.tif'))\n",
    "       \n",
    "        # Save the max projection image to the new file path\n",
    "        Image.fromarray(mean_image).save(max_proj_image_path)\n",
    "\n",
    "        # Return the path to the saved image\n",
    "        return max_proj_image_path\n",
    "    \n",
    "    def analyze_all_sessions(self, function_to_apply):\n",
    "        \"\"\"\n",
    "        Iterates over all session IDs in the directory DataFrame and applies the given function to each.\n",
    "\n",
    "        Parameters:\n",
    "        function_to_apply (callable): Function to be applied to each session. It should accept a session ID.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with session_ids as keys and function return values as values.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for session_id in self.directory_df['session_id']:\n",
    "            try:\n",
    "                result = function_to_apply(session_id)\n",
    "                results[session_id] = result\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing session {session_id}: {e}\")\n",
    "        return results\n",
    "    \n",
    "    def analyze_session_max_projection(self, session_id):\n",
    "        \"\"\"\n",
    "        Wrapper function to apply max_projection_mean_values to a session's TIF file.\n",
    "\n",
    "        Parameters:\n",
    "        session_id (str): The session ID for which the TIF file will be processed.\n",
    "\n",
    "        Returns:\n",
    "        str: Path to the processed max projection TIFF file.\n",
    "        \"\"\"\n",
    "        # analysis is an instance of ImageAnalysis\n",
    "        tif_path = self.get_session_raw_data(session_id)\n",
    "        if isinstance(tif_path, str) and tif_path.endswith('.tif'):\n",
    "            return self.max_projection_mean_values(tif_path)\n",
    "        else:\n",
    "            return f\"No valid TIF file found for session {session_id}\"# Apply max_projection_mean_values to all sessions\n",
    "        \n",
    "    def get_session_raw_data(self, session_id):\n",
    "        # Check if the session_id is in the 'session_id' column of the directory_df\n",
    "        if session_id in self.directory_df['session_id'].tolist():\n",
    "            # Find the directory path for the given session_id\n",
    "            directory_path = self.directory_df[self.directory_df['session_id'] == session_id]['directory_path'].values[0]\n",
    "            \n",
    "            # Search for the .tif file within that directory\n",
    "            for file_name in os.listdir(directory_path):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    return os.path.join(directory_path, file_name)\n",
    "\n",
    "            # If no .tif file is found in the directory\n",
    "            return f\"No .tif file found in the directory for session {session_id}.\"\n",
    "        else:\n",
    "            # If the session_id is not present in the DataFrame\n",
    "            return f\"Session ID {session_id} is not present in the directory DataFrame.\"\n",
    "         \n",
    "    def add_tiff_dimensions(self):\n",
    "        \"\"\"\n",
    "        Analyzes the dimensions of TIF files in the directory DataFrame and adds this data as new columns.\n",
    "        \"\"\"\n",
    "        # Ensure the DataFrame has the columns for dimensions\n",
    "        if 'x_dim' not in self.directory_df.columns:\n",
    "            self.directory_df['x_dim'] = None\n",
    "            self.directory_df['y_dim'] = None\n",
    "            self.directory_df['z_dim_frames'] = None\n",
    "\n",
    "        # Iterate over each session_id and update the dimensions\n",
    "        for index, row in self.directory_df.iterrows():\n",
    "            tif_path = self.get_session_raw_data(row['session_id'])\n",
    "            if isinstance(tif_path, str) and tif_path.endswith('.tif'):\n",
    "                try:\n",
    "                    with Image.open(tif_path) as img:\n",
    "                        self.directory_df.at[index, 'x_dim'] = img.width\n",
    "                        self.directory_df.at[index, 'y_dim'] = img.height\n",
    "                        # For z-dimension, count the frames\n",
    "                        img.seek(0)  # Ensure the pointer is at the beginning\n",
    "                        frames = 0\n",
    "                        while True:\n",
    "                            try:\n",
    "                                img.seek(img.tell() + 1)\n",
    "                                frames += 1\n",
    "                            except EOFError:\n",
    "                                break\n",
    "                        self.directory_df.at[index, 'z_dim_frames'] = frames\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process TIF dimensions for session {row['session_id']}: {e}\")\n",
    "                                \n",
    "    def analyze_all_calcium_signals(self):\n",
    "        \"\"\"\n",
    "        Applies calcium signal extraction to all session_ids in the directory DataFrame and stores the results.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for session_id in self.directory_df['session_id']:\n",
    "            # Ensure the ROI analysis has been done to get the labeled images\n",
    "            roi_results = self.analyze_roi(session_id)\n",
    "            # Check if analyze_roi returned a path to labeled images\n",
    "            if isinstance(roi_results, tuple):\n",
    "                # Extract calcium signals using the labeled ROI mask\n",
    "                calcium_csv_path = self.extract_calcium_signals(session_id)\n",
    "                results[session_id] = calcium_csv_path\n",
    "            else:\n",
    "                # If roi_results is an error message, pass it through\n",
    "                results[session_id] = roi_results\n",
    "                \n",
    "            #results is a dictionary where each key is a session_id and the corresponding value is the path to the saved CSV file containing calcium signal data.\n",
    "        return results\n",
    "     \n",
    "    def analyze_roi(self, session_id):\n",
    "        \"\"\"\n",
    "        Analyzes ROI of the 'labels_postexport.tif' file for a given session and saves two results:\n",
    "        one with labels and another without labels.I t also saves the labeled image data as numpy array for future use.\n",
    "        \"\"\"\n",
    "        \n",
    "        #### SETP 1: DEFINE PATHS ####\n",
    "        # define the paths, including the directory where processed images will be saved (processed_dir) \n",
    "        # and the name of the TIF file that contains the ROI labels (consistent_file_name)\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        consistent_file_name = 'labels_postexport.tif'\n",
    "        output_suffix_with_labels = '_roi_analysis_with_labels.png'\n",
    "        output_suffix_without_labels = '_roi_analysis_without_labels.png'\n",
    "\n",
    "        #### STEP 2: RETRIEVE SESSION DATA ####\n",
    "        # Retrieve the directory path from the DataFrame\n",
    "        # looks up the session's directory path from a DataFrame (directory_df) using the provided session_id. \n",
    "        # If the session ID isn't found, it returns a message indicating no directory entry was found for that session.\n",
    "        \n",
    "        directory_entry = self.directory_df[self.directory_df['session_id'] == session_id]\n",
    "        if directory_entry.empty:\n",
    "            return f\"No directory entry found for session {session_id}\"\n",
    "\n",
    "        #### STEP 3: VERIFY AND LOAD THE ROI TIF FILE ####\n",
    "        # constructs the full path to the labels_postexport.tif file and checks if it exists. If it does, the file is opened and loaded. \n",
    "        # If the file is in RGB format, it's converted to grayscale using rgb2gray from skimage.color. \n",
    "        # This conversion is crucial for analyzing the image as a binary mask where non-white pixels are considered ROIs.\n",
    "        directory_path = directory_entry['directory_path'].values[0]\n",
    "        \n",
    "        # Build the path to the postexport TIFF file\n",
    "        tiff_file_path = os.path.join(directory_path, processed_dir, consistent_file_name)\n",
    "\n",
    "        # Verify that the file exists\n",
    "        if not os.path.exists(tiff_file_path):\n",
    "            return f\"File not found for session {session_id}\"\n",
    "        else:\n",
    "            print(f\"Analyzing session {session_id}...\")\n",
    "        \n",
    "        #### STEP 4: CREATE AND SAVE THE BINARY MASK ####\n",
    "        # k: The method then converts the grayscale image to a binary mask, identifying all non-white pixels as ROIs \n",
    "        # (pixels with value less than 1 after normalization are set to 1, and others to 0). \n",
    "        # This binary mask is labeled using label from skimage.measure, assigning a unique label to each connected component (ROI).\n",
    "        \n",
    "        # Load the image\n",
    "        mask_image = Image.open(tiff_file_path)\n",
    "\n",
    "        # Convert RGB image to grayscale if necessary\n",
    "        if mask_image.mode == 'RGB':\n",
    "            # Convert to grayscale using skimage's rgb2gray\n",
    "            image_array = rgb2gray(np.array(mask_image))\n",
    "\n",
    "        # Assuming that all non-white pixels are ROIs\n",
    "        binary_mask = np.where(image_array < 1, 1, 0)  # Here, 1 corresponds to white in the normalized grayscale image\n",
    "\n",
    "        # Label the regions\n",
    "        labeled_image = label(binary_mask, connectivity=1)\n",
    "        num_rois = np.max(labeled_image)\n",
    "        \n",
    "        # Save the labeled image data as a NumPy array file for future processing\n",
    "        labeled_image_path = os.path.join(directory_path, processed_dir, f\"{session_id}_labeled_image.npy\")\n",
    "        np.save(labeled_image_path, labeled_image)\n",
    "        \n",
    "        \n",
    "        #### STEP 5: SAVE THE UNLABELED ROI IMAGE ####\n",
    "        # Save Unlabeled ROI Image: The method saves a version of the labeled image without any annotations to a specified path (output_path_without_labels). \n",
    "        # This image is saved in the processed_image_analysis_output directory with a specific suffix to indicate it's the unlabeled version.\n",
    "        \n",
    "        # Save the image without labels\n",
    "        output_path_without_labels = os.path.join(directory_path, processed_dir, session_id + output_suffix_without_labels)\n",
    "        plt.imsave(output_path_without_labels, labeled_image, cmap='nipy_spectral')\n",
    "\n",
    "        \n",
    "        #### STEP 6: ANALYZE AND SAVE LABELED ROI IMAGE ####\n",
    "        # Iterates through each detected region using regionprops, extracts the centroid, \n",
    "        # and annotates the image with the region's label. \n",
    "        # This annotated image is saved separately, indicating it includes ROI labels.\n",
    "        \n",
    "        # Analyze regions and save properties\n",
    "        regions = regionprops(labeled_image)\n",
    "\n",
    "        # Prepare to save the ROI analysis image with labels\n",
    "        output_path_with_labels = os.path.join(directory_path, processed_dir, session_id + output_suffix_with_labels)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(labeled_image, cmap='nipy_spectral')\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Annotate each ROI with its corresponding label (ID)\n",
    "        for region in regions:\n",
    "            # Get the coordinates of the centroid of the region\n",
    "            y, x = region.centroid\n",
    "            # Annotate the ROI ID at the centroid position\n",
    "            ax.text(x, y, str(region.label), color='white', ha='center', va='center')\n",
    "\n",
    "        plt.savefig(output_path_with_labels)\n",
    "        plt.close()\n",
    "\n",
    "        # Return the paths of the saved figures LABELED AND UNLABELED and number of ROIs\n",
    "        return (output_path_with_labels, output_path_without_labels), num_rois\n",
    "    \n",
    "    def analyze_all_rois(self):\n",
    "        \"\"\"\n",
    "        Applies ROI analysis to all sessions and saves the results.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for session_id in self.directory_df['session_id']:\n",
    "            result = self.analyze_roi(session_id)\n",
    "            results[session_id] = result\n",
    "        return results\n",
    "    \n",
    "    def process_all_sessions(self, use_corrected_data=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        This assumes the analyze_all_rois method has been previously run to generate the numpy files \n",
    "        and the corresponding images with and without labels for ROI per session.\n",
    "        \n",
    "        Process all sessions using either corrected or uncorrected calcium signal data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        use_corrected_data : bool, optional\n",
    "            Flag indicating whether to use corrected calcium signals. Defaults to False, \n",
    "            indicating uncorrected data should be used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary with processed data for all sessions, keyed by session ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        all_data = {}\n",
    "        \n",
    "        for session_id in self.directory_df['session_id'].unique():\n",
    "            stim_frame_numbers, roi_data, stimulation_ids = self.create_trial_locked_calcium_signals(session_id, use_corrected_data=use_corrected_data)\n",
    "            if stim_frame_numbers and roi_data and stimulation_ids:  # Ensure data was returned\n",
    "                all_data[session_id] = {\n",
    "                    'stim_frame_numbers': stim_frame_numbers,\n",
    "                    'roi_data': roi_data,\n",
    "                    'stimulation_ids': stimulation_ids\n",
    "                }\n",
    "        return all_data\n",
    "    \n",
    "    def create_trial_locked_calcium_signals(self, session_id, use_corrected_data=False):\n",
    "        \"\"\"\n",
    "        Generate trial-locked calcium signal data for a given session ID, allowing\n",
    "        the choice between corrected and uncorrected data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        session_id : str\n",
    "            The session ID for which to generate trial-locked signals.\n",
    "        use_corrected_data : bool, optional\n",
    "            Whether to use corrected calcium signal data. The default is False, which uses uncorrected data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the stimulation frame numbers, ROI data, and stimulation IDs.\n",
    "        \"\"\"\n",
    "\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        calcium_csv_suffix = '_corrected_calcium_signals.csv' if use_corrected_data else '_calcium_signals.csv'\n",
    "\n",
    "        directory_entry = self.directory_df[self.directory_df['session_id'] == session_id]\n",
    "\n",
    "        if directory_entry.empty:\n",
    "            print(f\"No directory entry found for session {session_id}\")\n",
    "            return None, None, None\n",
    "\n",
    "        directory_path = directory_entry['directory_path'].values[0]\n",
    "        csv_path = os.path.join(directory_path, processed_dir, f\"{session_id}{calcium_csv_suffix}\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Calcium signals file not found for session {session_id} using {'corrected' if use_corrected_data else 'uncorrected'} data\")\n",
    "            return None, None, None\n",
    "\n",
    "        calcium_signals_df = pd.read_csv(csv_path)\n",
    "        stim_frame_numbers = directory_entry['stimulation_frame_number'].values[0]\n",
    "        stimulation_ids = directory_entry['stimulation_ids'].values[0]\n",
    "\n",
    "        pre_stim_frames = 10 # 10 frames before stimulation\n",
    "        post_stim_frames = 100 # 100 frames after stimulation\n",
    "\n",
    "        roi_data = {roi: {} for roi in calcium_signals_df.columns if 'ROI' in roi}\n",
    "\n",
    "        for stim_id, stim_frame in zip(stimulation_ids, stim_frame_numbers):\n",
    "            start_idx = max(stim_frame - pre_stim_frames, 0) \n",
    "            end_idx = min(stim_frame + post_stim_frames, len(calcium_signals_df))\n",
    "\n",
    "            for roi in roi_data:\n",
    "                trial = calcium_signals_df.loc[start_idx:end_idx, roi]\n",
    "                roi_data[roi][(stim_id, stim_frame)] = trial.to_numpy()\n",
    "\n",
    "        return stim_frame_numbers, roi_data, stimulation_ids\n",
    "    \n",
    "    def extract_calcium_signals(self, session_id):\n",
    "        \"\"\"\n",
    "        Extracts calcium signals from time-series data using the saved labeled ROI mask\n",
    "        and saves the results as a CSV file in the 'processed_image_analysis_output' directory.\n",
    "\n",
    "        Parameters:\n",
    "        session_id (str): Session ID for which to perform the analysis.\n",
    "\n",
    "        Returns:\n",
    "        str: Path to the saved CSV file containing calcium signal data.\n",
    "        \"\"\"\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        calcium_csv_suffix = '_calcium_signals.csv'\n",
    "\n",
    "        # Retrieve the directory path from the DataFrame\n",
    "        directory_entry = self.directory_df[self.directory_df['session_id'] == session_id]\n",
    "        if directory_entry.empty:\n",
    "            return f\"No directory entry found for session {session_id}\"\n",
    "\n",
    "        directory_path = directory_entry['directory_path'].values[0]\n",
    "\n",
    "        # Path to the saved labeled image numpy file\n",
    "        labeled_image_path = os.path.join(directory_path, processed_dir, session_id + '_labeled_image.npy')\n",
    "\n",
    "        # Verify and load the labeled image numpy file\n",
    "        if not os.path.exists(labeled_image_path):\n",
    "            return f\"Labeled image file not found for session {session_id}\"\n",
    "        labeled_image = np.load(labeled_image_path)\n",
    "\n",
    "        # Locate and load the time-series TIFF file\n",
    "        tif_files = glob.glob(os.path.join(directory_path, '*.tif'))\n",
    "        tif_files = [f for f in tif_files if 'postexport' not in f and 'labels' not in f]  # Ensure it's the correct TIFF\n",
    "        if not tif_files:\n",
    "            return f\"No time-series .tif file found in the directory for session {session_id}\"\n",
    "        time_series_path = tif_files[0]  # Assuming there's only one relevant TIFF file\n",
    "        time_series = io.imread(time_series_path)\n",
    "\n",
    "        # Initialize an array to store calcium signal data\n",
    "        num_rois = np.max(labeled_image)\n",
    "        num_frames = time_series.shape[0]\n",
    "        calcium_signals = np.zeros((num_rois, num_frames))\n",
    "\n",
    "        # Extract the signal from each ROI in each frame\n",
    "        for t in range(num_frames):\n",
    "            frame = time_series[t]\n",
    "            for roi in range(1, num_rois + 1):\n",
    "                roi_mask = labeled_image == roi\n",
    "                roi_data = frame[roi_mask]\n",
    "                calcium_signals[roi - 1, t] = np.mean(roi_data)\n",
    "\n",
    "        # Create and save the DataFrame with calcium signals\n",
    "        calcium_df = pd.DataFrame(calcium_signals.T, columns=[f\"ROI_{i}\" for i in range(1, num_rois + 1)])\n",
    "        calcium_df['Frame'] = np.arange(1, num_frames + 1)\n",
    "        csv_path = os.path.join(directory_path, processed_dir, session_id + calcium_csv_suffix)\n",
    "        calcium_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        return csv_path\n",
    "    \n",
    "    def process_all_sessions_entire_recording(self, use_corrected_data=False):\n",
    "        \"\"\"\n",
    "        Processes all sessions and stores calcium signal dataframes in a dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        use_corrected_data : bool, optional\n",
    "            Whether to use corrected calcium signal data. The default is False, which uses uncorrected data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary where each key is a session ID and the value is the corresponding calcium_signals dataframe.\n",
    "        \"\"\"\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        calcium_csv_suffix = '_corrected_calcium_signals.csv' if use_corrected_data else '_calcium_signals.csv'\n",
    "        session_data = {}\n",
    "\n",
    "        for session_id in self.directory_df['session_id'].unique():\n",
    "            directory_entry = self.directory_df[self.directory_df['session_id'] == session_id]\n",
    "\n",
    "            if directory_entry.empty:\n",
    "                print(f\"No directory entry found for session {session_id}\")\n",
    "                continue  # Skip this session and proceed with the next\n",
    "\n",
    "            directory_path = directory_entry['directory_path'].values[0]\n",
    "            csv_path = os.path.join(directory_path, processed_dir, f\"{session_id}{calcium_csv_suffix}\")\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"Calcium signals file not found for session {session_id} using {'corrected' if use_corrected_data else 'uncorrected'} data\")\n",
    "                continue  # Skip this session and proceed with the next\n",
    "\n",
    "            calcium_signals_df = pd.read_csv(csv_path)\n",
    "            # Store the dataframe in the dictionary with session_id as the key\n",
    "            session_data[session_id] = calcium_signals_df\n",
    "\n",
    "        return session_data\n",
    "        \n",
    "    def calculate_responsiveness(self, all_data, pre_stim_frames=10, post_stim_frames=20, alpha=0.05, return_dataframe=False):    \n",
    "        \"\"\"\n",
    "        This function calculates and identifies responsive cells within calcium imaging data, applying statistical \n",
    "        tests to determine whether the change in signal post-stimulation is significant compared to the pre-stimulation \n",
    "        baseline. It stores detailed metrics including means, standard deviations, and p-values for each ROI across all sessions.\n",
    "\n",
    "        Parameters:\n",
    "        - all_data (dict): Nested dictionary containing the processed calcium signal data for multiple sessions, \n",
    "        structured with session IDs as top-level keys.\n",
    "        - pre_stim_frames (int): The number of frames before the stimulus used to calculate the baseline signal.\n",
    "        - post_stim_frames (int): The number of frames after the stimulus used for post-stimulus signal analysis.\n",
    "        - alpha (float): The significance level used to determine if a response is statistically significant.\n",
    "        - return_dataframe (bool): If set to True, the function also returns a pandas DataFrame containing the computed metrics.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A nested dictionary containing calculated metrics for each session ID, ROI, and stimulus event. If \n",
    "        `return_dataframe` is True, it also returns a DataFrame alongside this dictionary.\n",
    "\n",
    "        The output dictionary follows a multi-level structure:\n",
    "        - Level 1 (Session Level): Keys are session IDs, and values are dictionaries containing data for each session.\n",
    "        - Level 2 (ROI Level): Within each session dictionary, keys are ROIs, and values are dictionaries with metrics for each ROI.\n",
    "        - Level 3 (Stimulus Event Level): For each ROI, keys are tuples of (stimulation_id, stim_frame_number), and values \n",
    "        are dictionaries containing the metrics calculated for each stimulus event.\n",
    "\n",
    "        Metrics included for each stimulus event:\n",
    "        - 'pre_stim_mean': Mean of the signal in the pre-stimulus period.\n",
    "        - 'pre_stim_sd': Standard deviation of the signal in the pre-stimulus period.\n",
    "        - 'post_stim_peak': Maximum signal value in the post-stimulus period (not normalized).\n",
    "        - 'post_stim_sd': Standard deviation of the signal in the post-stimulus period, excluding the peak value.\n",
    "        - 'p_value': P-value from the t-test comparing pre-stimulus and post-stimulus signals.\n",
    "        - 'is_responsive': Boolean indicating whether the ROI is considered responsive based on the p-value being below alpha.\n",
    "\n",
    "        \n",
    "        Returns:\n",
    "        dict or (dict, pd.DataFrame): A dictionary and optionally a DataFrame containing all metrics and SDs for each session ID, ROI, and stimulus.\n",
    "        \"\"\"\n",
    "        responsiveness_data = {}\n",
    "        dataframe_rows = []\n",
    "\n",
    "        for session_id, session_data in all_data.items():\n",
    "            session_responsiveness = {}\n",
    "            for roi, roi_data in session_data['roi_data'].items():\n",
    "                roi_responsiveness = {}\n",
    "                for (stim_id, stim_frame), signal_data in roi_data.items():\n",
    "                    # Validate signal_data length\n",
    "                    if signal_data.size >= (pre_stim_frames + post_stim_frames + 1):\n",
    "                        pre_stim_signal = signal_data[:pre_stim_frames]\n",
    "                        post_stim_signal = signal_data[pre_stim_frames + 1 : pre_stim_frames + 1 + post_stim_frames]\n",
    "                        \n",
    "    \n",
    "                        \n",
    "                        # New calculation for the entire array\n",
    "                        delta_f_f_full_array = (signal_data - np.mean(signal_data[:pre_stim_frames])) / np.mean(signal_data[:pre_stim_frames])\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        # Calculate means and SDs\n",
    "                        pre_stim_mean = np.mean(pre_stim_signal)\n",
    "                        pre_stim_sd = np.std(pre_stim_signal)\n",
    "                        post_stim_peak = np.nanmax(post_stim_signal) if not np.isnan(np.nanmax(post_stim_signal)) else np.nan\n",
    "                        post_stim_sd = np.std(post_stim_signal[1:])  # Excluding the peak (stimulation point)\n",
    "                        post_stim_peak_index = np.nanargmax(post_stim_signal) if not np.isnan(post_stim_peak) else np.nan\n",
    "                        \n",
    "                        #calculate the median of the post_stim_signal and the median of the pre_stim_signal\n",
    "                        post_stim_median = np.median(post_stim_signal)\n",
    "                        pre_stim_median = np.median(pre_stim_signal)\n",
    "                        \n",
    "                        #calculate the mean of the post_stim_signal and the mean of the pre_stim_signal\n",
    "                        post_stim_mean = np.mean(post_stim_signal)\n",
    "                        \n",
    "                        # calculate the delta_f/f for the post_stim_signal and the pre_stim_signal and save entire array\n",
    "                        delta_f_f_post_stim = (post_stim_signal - pre_stim_mean) / pre_stim_mean\n",
    "                        \n",
    "                        # calculate the peak delta_f/f for the post_stim_signal and save the value \n",
    "                        peak_delta_f_f_post_stim = (post_stim_peak - pre_stim_mean) / pre_stim_mean\n",
    "                \n",
    "                        # Perform t-test between normalized pre-stimulus and post-stimulus signals\n",
    "                        t_stat, p_value = ttest_ind(pre_stim_signal, post_stim_signal, equal_var=False)\n",
    "\n",
    "                        # Determine responsiveness based on the p-value without explicit prior length check\n",
    "                        is_responsive = p_value < alpha if not np.isnan(p_value) else False\n",
    "                        \n",
    "                        # Time metrics calculations with safety checks\n",
    "                        half_peak_value = post_stim_peak / 2 if not np.isnan(post_stim_peak) else np.nan\n",
    "                        half_rise_index = np.where(post_stim_signal >= half_peak_value)[0][0] if np.any(post_stim_signal >= half_peak_value) else np.nan\n",
    "                        half_decay_index = np.where(post_stim_signal[post_stim_peak_index:] <= half_peak_value)[0][0] + post_stim_peak_index if post_stim_peak_index and np.any(post_stim_signal[post_stim_peak_index:] <= half_peak_value) else np.nan\n",
    "\n",
    "                        # Convert indices to milliseconds\n",
    "                        # Adjusted line with conditional to ensure a minimum of 100 ms:\n",
    "                        time_to_peak = max(100, post_stim_peak_index * 100) if not np.isnan(post_stim_peak_index) else np.nan\n",
    "                        half_rise_time = half_rise_index * 100 if not np.isnan(half_rise_index) else np.nan\n",
    "                        half_decay_time = half_decay_index * 100 if not np.isnan(half_decay_index) else np.nan\n",
    "\n",
    "                    # Save all calculated metrics\n",
    "                    roi_responsiveness[(stim_id, stim_frame)] = {\n",
    "                        'pre_stim_mean': pre_stim_mean,\n",
    "                        'pre_stim_sd': pre_stim_sd,\n",
    "                        'post_stim_peak': post_stim_peak,\n",
    "                        'post_stim_sd': post_stim_sd,\n",
    "                        'p_value': p_value,\n",
    "                        'post_stim_mean': post_stim_mean,\n",
    "                        'delta_f_f_post_stim': delta_f_f_post_stim,\n",
    "                        'pre_stim_median': pre_stim_median,\n",
    "                        'post_stim_median': post_stim_median,\n",
    "                        'peak_delta_f_f_post_stim': peak_delta_f_f_post_stim,\n",
    "                        'is_responsive': is_responsive\n",
    "                \n",
    "                    }\n",
    "\n",
    "                    # Append data for DataFrame\n",
    "                    dataframe_rows.append({\n",
    "                        'session_id': session_id,\n",
    "                        'roi': roi,\n",
    "                        'stimulation_id': stim_id,\n",
    "                        'stim_frame_number': stim_frame,\n",
    "                        'pre_stim_mean': pre_stim_mean,\n",
    "                        'pre_stim_sd': pre_stim_sd,\n",
    "                        'post_stim_peak': post_stim_peak,\n",
    "                        'post_stim_sd': post_stim_sd,\n",
    "                        'post_stim_mean': post_stim_mean,\n",
    "                        'delta_f_f_post_stim': delta_f_f_post_stim,\n",
    "                        'pre_stim_median': pre_stim_median,\n",
    "                        'post_stim_median': post_stim_median,\n",
    "                        'peak_delta_f_f_post_stim': peak_delta_f_f_post_stim,\n",
    "                        'delta_f_f_full_array': delta_f_f_full_array,\n",
    "                        'raw_signal': signal_data,\n",
    "                        'p_value': p_value,\n",
    "                        'time_to_peak': time_to_peak,\n",
    "                        'half_rise_time': half_rise_time,\n",
    "                        'half_decay_time': half_decay_time,\n",
    "                        'is_responsive': is_responsive\n",
    "                    })\n",
    "\n",
    "                session_responsiveness[roi] = roi_responsiveness\n",
    "            responsiveness_data[session_id] = session_responsiveness\n",
    "\n",
    "        # Create and return DataFrame if requested\n",
    "        if return_dataframe:\n",
    "            responsiveness_df = pd.DataFrame(dataframe_rows)\n",
    "            return responsiveness_data, responsiveness_df\n",
    "        else:\n",
    "            return responsiveness_data\n",
    "        \n",
    "    def filter_responsive_rois(self, all_data, responsiveness_data):\n",
    "        \"\"\"\n",
    "        Creates a new data structure similar to all_data but excludes the data for non-responsive ROIs \n",
    "        for specific stimulation IDs, maintaining only responsive ROI data.\n",
    "\n",
    "        Parameters:\n",
    "        all_data (dict): Original dictionary with the complete dataset.\n",
    "        responsiveness_data (dict): Dictionary containing responsiveness information for each ROI.\n",
    "\n",
    "        Returns:\n",
    "        dict: A new dictionary mirroring all_data's structure but excluding data for non-responsive ROIs per stimulus.\n",
    "        \"\"\"\n",
    "        filtered_data = {}\n",
    "\n",
    "        for session_id, session_content in all_data.items():\n",
    "            filtered_data[session_id] = {\n",
    "                'stim_frame_numbers': session_content['stim_frame_numbers'],\n",
    "                'roi_data': {},\n",
    "                'stimulation_ids': session_content['stimulation_ids']\n",
    "            }\n",
    "\n",
    "            for roi, roi_data in session_content['roi_data'].items():\n",
    "                filtered_roi_data = {}\n",
    "\n",
    "                for stim_key, signal_data in roi_data.items():\n",
    "                    # Include the data only if the ROI is responsive for this stimulus\n",
    "                    if responsiveness_data[session_id][roi].get(stim_key, {}).get('is_responsive', False):\n",
    "                        filtered_roi_data[stim_key] = signal_data\n",
    "                \n",
    "                # Update only if there's at least one responsive stim event for the ROI\n",
    "                if filtered_roi_data:\n",
    "                    filtered_data[session_id]['roi_data'][roi] = filtered_roi_data\n",
    "\n",
    "        return filtered_data\n",
    "    \n",
    "    def filter_responsive_rois_by_stimulation(self, session_data, responsiveness_df):\n",
    "        # Initialize a dictionary to hold the filtered dataframes\n",
    "        filtered_data_by_session = {}\n",
    "        \n",
    "        # Filter for responsive ROIs with stimulation_id == 12\n",
    "        responsive_df = responsiveness_df[\n",
    "            (responsiveness_df['is_responsive']) & \n",
    "            (responsiveness_df['stimulation_id'] == 12)\n",
    "        ]\n",
    "        \n",
    "        # Group by session_id to process each session separately\n",
    "        grouped_responsive_df = responsive_df.groupby('session_id')\n",
    "        \n",
    "        for session_id, group in grouped_responsive_df:\n",
    "            # Initialize a list to collect dataframes for this session\n",
    "            session_frames_list = []\n",
    "            \n",
    "            # Get unique ROIs for this session that are responsive\n",
    "            unique_rois = group['roi'].unique()\n",
    "            \n",
    "            # Access the session's dataframe\n",
    "            session_df = session_data.get(session_id)\n",
    "            if session_df is None:\n",
    "                print(f\"Session ID {session_id} not found in session_data.\")\n",
    "                continue\n",
    "            \n",
    "            # Filter the session dataframe for responsive ROIs\n",
    "            for roi in unique_rois:\n",
    "                # Extract the ROI number and construct the column name\n",
    "                roi_number = re.search(r'\\d+', roi)\n",
    "                if not roi_number:\n",
    "                    print(f\"ROI format is incorrect for {roi}\")\n",
    "                    continue\n",
    "                roi_column_name = f'ROI_{roi_number.group()}'\n",
    "                \n",
    "                if roi_column_name in session_df.columns:\n",
    "                    # Access the entire column for the responsive ROI\n",
    "                    roi_frames_df = session_df[[roi_column_name]].copy()\n",
    "                    \n",
    "                    # Add the ROI frames to the list for this session\n",
    "                    session_frames_list.append(roi_frames_df)\n",
    "                else:\n",
    "                    print(f\"Column {roi_column_name} not found in session dataframe for session_id {session_id}.\")\n",
    "            \n",
    "            # Combine the frames for the session into a single dataframe\n",
    "            if session_frames_list:\n",
    "                combined_frames_df = pd.concat(session_frames_list, axis=1)\n",
    "                # Store the filtered data in the dictionary using the session_id as the key\n",
    "                filtered_data_by_session[session_id] = combined_frames_df\n",
    "\n",
    "        return filtered_data_by_session\n",
    "    \n",
    "    def plot_session_time_series(self, filtered_data_by_session):\n",
    "        for session_id, session_df in filtered_data_by_session.items():\n",
    "            # Calculate the nanmean signal across ROIs for the session\n",
    "            median_signal = session_df.median(axis=1, skipna=True)\n",
    "            \n",
    "            # Setup the plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"Session ID: {session_id} Entire Recording \")\n",
    "            plt.xlabel(\"Time (frames)\")\n",
    "            plt.ylabel(\"Signal (a.u.)\") \n",
    "            \n",
    "            # Plot each ROI time series in grey\n",
    "            for column in session_df.columns:\n",
    "                plt.plot(session_df.index, session_df[column], color='lightgrey', alpha=0.5, lw=0.5)\n",
    "            \n",
    "            # Plot the nanmean signal in blue\n",
    "            plt.plot(session_df.index, median_signal, color='blue', label='Median Signal')\n",
    "            \n",
    "            # Add legend\n",
    "            plt.legend()\n",
    "            \n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "\n",
    "    def process_biolumi_calcium_signal(self, session_id, directory_df):\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        calcium_csv_suffix = '_calcium_signals.csv'\n",
    "        directory_entry = directory_df[directory_df['session_id'] == session_id]\n",
    "\n",
    "        # Check if the directory_entry is empty\n",
    "        if directory_entry.empty:\n",
    "            print(f\"No directory entry found for session {session_id}. Please check the session_id.\")\n",
    "            return None\n",
    "        \n",
    "        directory_path = directory_entry['directory_path'].values[0]\n",
    "        csv_path = os.path.join(directory_path, processed_dir, str(session_id) + calcium_csv_suffix)\n",
    "\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Calcium signals file not found for session {session_id}\")\n",
    "            return None\n",
    "\n",
    "        calcium_signals_df = pd.read_csv(csv_path) # import the calcium signals csv file\n",
    "        \n",
    "        # Correct the \"Dark signal\" for each ROI\n",
    "#        for roi in calcium_signals_df.columns:\n",
    "#            if 'ROI' in roi:  # Assuming ROI columns are prefixed with 'ROI'\n",
    "#                dark_signal_median = calcium_signals_df[roi][:300].median() # Calculate the median of the first 100 frames\n",
    "#                calcium_signals_df[roi] = calcium_signals_df[roi] - dark_signal_median\n",
    "#                calcium_signals_df.loc[calcium_signals_df[roi] < 0, roi] = np.nan\n",
    "        \n",
    "        # Calculate the grand mean across all ROIs for the first 300 frames\n",
    "        roi_columns = [roi for roi in calcium_signals_df.columns if 'ROI' in roi]  # Filter out ROI columns\n",
    "        grand_mean = calcium_signals_df[roi_columns].iloc[:300].mean().mean()  # Calculate the grand mean of the first 300 frames\n",
    "\n",
    "        # Subtract the grand mean from each ROI's calcium signal\n",
    "        for roi in roi_columns:\n",
    "            calcium_signals_df[roi] = calcium_signals_df[roi] - grand_mean\n",
    "            calcium_signals_df.loc[calcium_signals_df[roi] < 0, roi] = np.nan  # Set negative values to NaN\n",
    "       \n",
    "        \n",
    "        #save the corrected calcium signals to a new csv file in the same directory\n",
    "        corrected_csv_path = os.path.join(directory_path, processed_dir, str(session_id) + '_corrected' + calcium_csv_suffix)\n",
    "        calcium_signals_df.to_csv(corrected_csv_path, index=False)\n",
    "        \n",
    "        return calcium_signals_df\n",
    "\n",
    "    def process_all_sessions_biolumi(self):\n",
    "        unique_sessions = self.directory_df['session_id'].unique()\n",
    "        for session_id in unique_sessions:\n",
    "            print(f\"Processing session ID: {session_id}\")\n",
    "            self.process_biolumi_calcium_signal(session_id, self.directory_df)\n",
    "            print(f\"Completed processing for session ID: {session_id}\")\n",
    "            \n",
    "    def plot_stim_responsiveness(self, df, stim_ids=None, include='both', y_lim=None, x_lim=None, mean_color='black', figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Plots the delta F/F response for given stimulation IDs, filtering based on responsiveness if specified.\n",
    "        Individual replicates are plotted in light grey, while the mean response is plotted in a user-defined color.\n",
    "        Adds a red dotted line at the stimulation onset, considering the user-defined x-axis limits.\n",
    "        Prints the number of responsive and unresponsive units for each stimulus ID on the plot.\n",
    "        User can define the y-axis limits, x-axis limits, and the figure size.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): DataFrame containing the responsiveness data.\n",
    "        - stim_ids (list): List of stimulation IDs to plot. If None, all unique IDs in the DataFrame will be used.\n",
    "        - include (str): Can be 'responsive', 'non-responsive', or 'both' to filter units based on responsiveness.\n",
    "        - y_lim (tuple): A tuple of (min, max) for y-axis limits. If None, limits are automatically determined.\n",
    "        - x_lim (tuple): A tuple of (min, max) for x-axis limits. If None, defaults to the entire range of the data.\n",
    "        - mean_color (str): Color for the mean response line.\n",
    "        - figsize (tuple): Figure dimension as (width, height).\n",
    "\n",
    "        Returns:\n",
    "        - fig (plt.Figure): The created figure.\n",
    "        \"\"\"\n",
    "\n",
    "        # If stim_ids is not provided, get the unique IDs from the DataFrame and sort them\n",
    "        if stim_ids is None:\n",
    "            stim_ids = sorted(df['stimulation_id'].unique())\n",
    "        else:\n",
    "            stim_ids = sorted(stim_ids)\n",
    "        \n",
    "        # Adjust the x-axis to align with the pre-stimulus, stimulus onset, and post-stimulus periods\n",
    "        stim_index = 9  # Index at which stimulation occurs\n",
    "        total_frames = 111  # Total number of frames, including pre-stim, stim, and post-stim\n",
    "        sampling_interval = 100  # Time per index in ms at 10Hz sampling rate\n",
    "\n",
    "        # Create a figure and axes with subplots\n",
    "        n_stims = len(stim_ids)\n",
    "        fig, axes = plt.subplots(1, n_stims, figsize=figsize, sharey=True)\n",
    "\n",
    "        # Adjust if we only have one subplot to make sure 'axes' is iterable\n",
    "        if n_stims == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Set the y-axis limit if specified\n",
    "        if y_lim:\n",
    "            plt.setp(axes, ylim=y_lim)\n",
    "\n",
    "        # Set the x-axis limit if specified\n",
    "        if x_lim is not None:\n",
    "            new_x_lim = (x_lim[0] * sampling_interval, x_lim[1] * sampling_interval)\n",
    "            plt.setp(axes, xlim=new_x_lim)\n",
    "\n",
    "\n",
    "\n",
    "        for ax, stim_id in zip(axes, stim_ids):\n",
    "            # Filter the DataFrame based on the current stim_id\n",
    "            stim_df = df[df['stimulation_id'] == stim_id]\n",
    "            if include != 'both':\n",
    "                stim_df = stim_df[stim_df['is_responsive'] == (include == 'responsive')]\n",
    "\n",
    "            # Get the delta_f_f_full_array values for plotting\n",
    "            delta_f_f_values = np.vstack(stim_df['delta_f_f_full_array'].values)\n",
    "\n",
    "            # Calculate the time vector considering the stimulation index\n",
    "            time_vector = np.arange(-stim_index, total_frames - stim_index) * sampling_interval\n",
    "\n",
    "            # Plot individual replicates in light grey\n",
    "            for trace in delta_f_f_values:\n",
    "                ax.plot(time_vector, trace, color='lightgrey', linewidth=0.5)\n",
    "\n",
    "            # Calculate mean response and plot in the specified mean_color\n",
    "            mean_response = np.nanmedian(delta_f_f_values, axis=0)\n",
    "            ax.plot(time_vector, mean_response, color=mean_color, label=f'Stim ID {stim_id}')\n",
    "\n",
    "            # Add vertical line at stimulation onset if it's within the x-axis limits\n",
    "            if x_lim is None or (0 >= x_lim[0] and 0 <= x_lim[1]):\n",
    "                ax.axvline(x=0, color='red', linestyle='--', label='Stimulation Onset')\n",
    "\n",
    "            # Count and display the number of responsive and unresponsive units for this stim_id\n",
    "            num_responsive = len(stim_df[stim_df['is_responsive'] == True])\n",
    "            num_unresponsive = len(stim_df[stim_df['is_responsive'] == False])\n",
    "            info_text = f'Responsive: {num_responsive}'\n",
    "            ax.text(0.95, 0.95, info_text, transform=ax.transAxes, fontsize=9,\n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=dict(facecolor='white', alpha=0.5, edgecolor='black', boxstyle='round'))\n",
    "\n",
    "            # Set titles and labels\n",
    "            ax.set_title(f'Stim ID {stim_id}')\n",
    "            ax.set_xlabel('ms', fontsize=24)\n",
    "            ax.set_ylabel('ΔF/F$_o$', fontsize=24)\n",
    "            #make y-axis labels larger\n",
    "            ax.tick_params(axis='y', labelsize=18)\n",
    "            ax.tick_params(axis='x', labelsize=18)\n",
    "            ax.legend().remove()\n",
    "\n",
    "        # To prevent x-axis labels from overlapping\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def process_all_sessions_entire_recording_gcampbackgroundcorrected(self, sessions, rois_list, use_corrected_data=False):\n",
    "        \"\"\"\n",
    "        Processes specified sessions, performs background correction using specified ROIs, and overwrites the original CSV file\n",
    "        with the corrected data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sessions : list\n",
    "            A list of session IDs, e.g., ['2112232023'].\n",
    "        rois_list : list of lists\n",
    "            A list where each element is a list of ROIs to average and subtract for the corresponding session,\n",
    "            e.g., [['ROI_11', 'ROI_12', 'ROI_13']].\n",
    "        use_corrected_data : bool, optional\n",
    "            Whether to use corrected calcium signal data. The default is False, which uses uncorrected data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary where each key is a session ID and the value is the corresponding corrected calcium_signals dataframe.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        After performing the background correction, this method will overwrite the original CSV file with the corrected data\n",
    "        using the same file name and path. It replaces the file that was originally accessed.\n",
    "        \"\"\"\n",
    "        processed_dir = 'processed_data/processed_image_analysis_output'\n",
    "        calcium_csv_suffix = '_corrected_calcium_signals.csv' if use_corrected_data else '_calcium_signals.csv'\n",
    "        session_data = {}\n",
    "\n",
    "        for session_id, rois_to_average in zip(sessions, rois_list):\n",
    "            # Ensure the session ID exists in directory_df\n",
    "            if session_id not in self.directory_df['session_id'].unique():\n",
    "                print(f\"No directory entry found for session {session_id}\")\n",
    "                continue  # Skip this session and proceed with the next\n",
    "\n",
    "            # Find the corresponding directory path for the session\n",
    "            directory_entry = self.directory_df[self.directory_df['session_id'] == session_id]\n",
    "            directory_path = directory_entry['directory_path'].values[0]\n",
    "            csv_path = os.path.join(directory_path, processed_dir, f\"{session_id}{calcium_csv_suffix}\")\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"Calcium signals file not found for session {session_id} using {'corrected' if use_corrected_data else 'uncorrected'} data\")\n",
    "                continue  # Skip this session and proceed with the next\n",
    "\n",
    "            # Load the calcium signals data\n",
    "            calcium_signals_df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Print the number of columns before the operation\n",
    "            print(f\"Session {session_id}: Number of columns before operation: {calcium_signals_df.shape[1]}\")\n",
    "\n",
    "            # Check if the ROIs to average are present in the dataframe\n",
    "            if not all(roi in calcium_signals_df.columns for roi in rois_to_average):\n",
    "                print(f\"Some ROIs not found in session {session_id}: {rois_to_average}\")\n",
    "                continue  # Skip this session if any ROI is missing\n",
    "\n",
    "            # Calculate the mean of the specified ROIs (excluding the 'Frame' column)\n",
    "            rois_to_average_no_frame = [roi for roi in rois_to_average if roi != 'Frame']\n",
    "            roi_mean = calcium_signals_df[rois_to_average_no_frame].mean(axis=1)\n",
    "\n",
    "            # Subtract the mean from all columns except 'Frame'\n",
    "            columns_to_subtract = calcium_signals_df.columns.difference(['Frame'])\n",
    "            calcium_signals_corrected = calcium_signals_df.copy()\n",
    "            calcium_signals_corrected[columns_to_subtract] = calcium_signals_corrected[columns_to_subtract].subtract(roi_mean, axis=0)\n",
    "\n",
    "            # Drop the specified ROIs\n",
    "            calcium_signals_corrected = calcium_signals_corrected.drop(columns=rois_to_average_no_frame)\n",
    "\n",
    "            # Print the number of columns after the operation\n",
    "            print(f\"Session {session_id}: Number of columns after operation: {calcium_signals_corrected.shape[1]}\")\n",
    "\n",
    "            # Store the corrected dataframe in the session_data dictionary\n",
    "            session_data[session_id] = calcium_signals_corrected\n",
    "\n",
    "            # Overwrite the original CSV with the corrected data\n",
    "            calcium_signals_corrected.to_csv(csv_path, index=False)\n",
    "            print(f\"Session {session_id}: Corrected data saved and overwrote {csv_path}\")\n",
    "\n",
    "        return session_data\n",
    "    \n",
    "    def plot_session_time_series_for_specific_rois(self, filtered_data_by_session, rois_to_include, frame_range=None):\n",
    "        \"\"\"\n",
    "        Plots the time series for selected ROIs in each session with distinct colors for each ROI, \n",
    "        and converts frame numbers into time (seconds) on the x-axis based on 10Hz recording frequency.\n",
    "        :param filtered_data_by_session: A dictionary where keys are session IDs and values are DataFrames containing ROI time series data.\n",
    "        :param rois_to_include: A list of ROI names that should be included in the plot.\n",
    "        :param frame_range: A tuple (start_frame, end_frame) specifying the range of frames to plot. If None, plot all frames.\n",
    "        \"\"\"\n",
    "        # Define a color map to assign different colors for different ROIs\n",
    "        colors = plt.cm.get_cmap('tab10', len(rois_to_include))  # 'tab10' colormap for up to 10 distinct colors\n",
    "        \n",
    "        for session_id, session_df in filtered_data_by_session.items():\n",
    "            # Filter the DataFrame for the specified ROIs\n",
    "            session_df_filtered = session_df[session_df.columns.intersection(rois_to_include)]\n",
    "\n",
    "            if session_df_filtered.empty:\n",
    "                print(f\"No matching ROIs found in session {session_id} for the provided list of ROIs.\")\n",
    "                continue\n",
    "\n",
    "            # If a frame range is provided, slice the DataFrame to include only those frames\n",
    "            if frame_range is not None:\n",
    "                start_frame, end_frame = frame_range\n",
    "                session_df_filtered = session_df_filtered.iloc[start_frame:end_frame]\n",
    "            else:\n",
    "                start_frame, end_frame = 0, len(session_df_filtered)  # If no range is provided, use the full range\n",
    "\n",
    "            # Convert frame indices to time (seconds) using 10Hz frequency\n",
    "            time_vector = session_df_filtered.index / 10.0  # Convert frames to seconds (1 second = 10 frames)\n",
    "\n",
    "            # Setup the plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f\"Session ID: {session_id} - Selected ROIs\")\n",
    "            plt.xlabel(f\"Time (seconds)\")  # X-axis now shows time in seconds\n",
    "            plt.ylabel(\"Signal (a.u.)\")\n",
    "            \n",
    "            # Plot each selected ROI time series with its own color\n",
    "            for idx, column in enumerate(session_df_filtered.columns):\n",
    "                plt.plot(time_vector, session_df_filtered[column], \n",
    "                         color=colors(idx), label=f'{column}', lw=1.5)\n",
    "            \n",
    "            # Add legend to identify each ROI by its trace color\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "            # Show the plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "class SensorDataPlotter:\n",
    "    def __init__(self, data_frames, sensor_names, sensor_box_colors, sensor_strip_colors):\n",
    "        \"\"\"\n",
    "        Initialize the object with a list of data frames, corresponding sensor names, and specific colors for each sensor.\n",
    "        :param data_frames: List of pandas DataFrames containing the sensor data.\n",
    "        :param sensor_names: List of strings representing the names of the sensors.\n",
    "        :param sensor_box_colors: Dictionary mapping sensor names to boxplot colors.\n",
    "        :param sensor_strip_colors: Dictionary mapping sensor names to stripplot colors.\n",
    "        \"\"\"\n",
    "        self.data_frames = data_frames\n",
    "        self.sensor_names = sensor_names\n",
    "        self.sensor_box_colors = sensor_box_colors\n",
    "        self.sensor_strip_colors = sensor_strip_colors\n",
    "        self.combined_df = None\n",
    "        \n",
    "        # Call the method to add peak SNR column\n",
    "        self._add_peak_snr_column()\n",
    "    \n",
    "    def _add_peak_snr_column(self):\n",
    "        \"\"\"\n",
    "        Add a new column 'peak_snr' to each DataFrame in self.data_frames.\n",
    "        The peak SNR is calculated as peak_delta_f_f_post_stim divided by pre_stim_sd.\n",
    "        \"\"\"\n",
    "        for df in self.data_frames:\n",
    "            df['peak_snr'] = df['peak_delta_f_f_post_stim'] / df['pre_stim_sd']\n",
    "        \n",
    "    def prepare_for_plotting(self, df_column_name):\n",
    "        \"\"\"\n",
    "        Prepares a single dataframe suitable for plotting from multiple sensor dataframes.\n",
    "        :param df_column_name: The name of the column to use for the value in the plot.\n",
    "        \"\"\"\n",
    "        # Add a 'sensor_name' column to each DataFrame and concatenate them into a single DataFrame\n",
    "        frames = []\n",
    "        for df, name in zip(self.data_frames, self.sensor_names):\n",
    "            df = df.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "            df['sensor_name'] = name\n",
    "            df['value'] = df[df_column_name]\n",
    "            frames.append(df)\n",
    "        # Concatenate all Dataframes into a single DataFrame\n",
    "        self.combined_df = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "        #filter the self.combined_df to only include responsive ROIs when is_responsive is True based on if the entry is True or False\n",
    "        self.combined_df = self.combined_df[self.combined_df['is_responsive'] == True]\n",
    "        \n",
    "        # Ensure the value column is present and has the correct data type for plotting\n",
    "        if df_column_name not in self.combined_df.columns:\n",
    "            raise ValueError(f\"The column '{df_column_name}' does not exist in the DataFrame.\") # Raise an error if the column does not exist\n",
    "        self.combined_df[df_column_name] = pd.to_numeric(self.combined_df[df_column_name], errors='coerce') # Convert to numeric where eoors are coerced which means invalid parsing will be set as NaN\n",
    "    \n",
    "    def plot_data(self, df_column_name, selected_stim_ids, box_width=.8, strip_size=3, fig_size=(12, 8), dpi=300, save_dir=None, save_dpi=300):\n",
    "        \"\"\"\n",
    "        Plots the data using boxplot and stripplot for selected stimulation IDs.\n",
    "        :param df_column_name: The name of the column to use for the value in the plot.\n",
    "        :param selected_stim_ids: List of stimulation IDs to plot. If None, plot all.\n",
    "        :param box_width: The width of the boxplots.\n",
    "        :param strip_size: The size of the points in the stripplots.\n",
    "        :param fig_size: Tuple representing the figure size (width, height) in inches.\n",
    "        :param dpi: The resolution in dots per inch for display.\n",
    "        :param save_dir: Directory to save the plot. If None, the plot is not saved.\n",
    "        :param save_dpi: The resolution in dots per inch for saving the figure.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### check varuable types and raise errors if necessary ###\n",
    "        ##########################################################\n",
    "       \n",
    "        df_column_name = str(df_column_name)\n",
    "        \n",
    "        if selected_stim_ids is not None:\n",
    "            if not isinstance(selected_stim_ids, list):\n",
    "                raise ValueError(\"The selected_stim_ids parameter must be a list of stimulation IDs.\")\n",
    "            if not all(isinstance(stim_id, int) for stim_id in selected_stim_ids):\n",
    "                raise ValueError(\"All elements in the selected_stim_ids list must be integers.\")\n",
    "            \n",
    "        if not isinstance(box_width, (int, float)):\n",
    "            raise ValueError(\"The box_width parameter must be an integer or float.\")\n",
    "        if not isinstance(strip_size, (int, float)):\n",
    "            raise ValueError(\"The strip_size parameter must be an integer or float.\")\n",
    "        if not isinstance(fig_size, tuple) or len(fig_size) != 2:\n",
    "            raise ValueError(\"The fig_size parameter must be a tuple of two integers.\")\n",
    "        if not all(isinstance(val, (int, float)) for val in fig_size):\n",
    "            raise ValueError(\"The fig_size parameter must contain only integers or floats.\")\n",
    "        if not isinstance(dpi, int):\n",
    "            raise ValueError(\"The dpi parameter must be an integer.\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        ### import, and filter the combined_df for the selected stimulation IDs if provided###\n",
    "        #####################################################################################\n",
    "       \n",
    "        #check if the combined_df is None and if so, call the prepare_for_plotting method \n",
    "        if self.combined_df is None:\n",
    "            self.prepare_for_plotting(df_column_name)\n",
    "       \n",
    "            \n",
    "        # Filter the combined DataFrame for the selected stimulation IDs if provided\n",
    "        if selected_stim_ids is not None:\n",
    "            self.combined_df = self.combined_df[self.combined_df['stimulation_id'].isin(selected_stim_ids)]\n",
    "            \n",
    "        # Raise an error if no data is available after filtering\n",
    "        if self.combined_df.empty:\n",
    "            raise ValueError(\"No data available for the selected stimulation IDs.\")\n",
    "        \n",
    "        #### for debugging purposes #### \n",
    "        ################################\n",
    "            # Debugging: Print unique stimulation IDs in the combined_df\n",
    "        print(f\"Unique stimulation IDs in the combined dataset: {sorted(self.combined_df['stimulation_id'].unique())}\")\n",
    "        \n",
    "        if selected_stim_ids is not None:\n",
    "            # Debugging: Check which selected_stim_ids are present in the data\n",
    "            present_stim_ids = set(selected_stim_ids).intersection(set(self.combined_df['stimulation_id']))\n",
    "            missing_stim_ids = set(selected_stim_ids) - present_stim_ids\n",
    "            print(f\"Stimulation IDs present in the data: {sorted(present_stim_ids)}\")\n",
    "            print(f\"Stimulation IDs not found in the data: {sorted(missing_stim_ids)}\")\n",
    "            \n",
    "            self.combined_df = self.combined_df[self.combined_df['stimulation_id'].isin(selected_stim_ids)]\n",
    "        \n",
    "        \n",
    "        ### set up the boxplot and stripplot properties ###\n",
    "        ####################################################\n",
    "        \n",
    "        # Boxplot properties will have black edges and lines\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 1.5}\n",
    "        lineprops = {'color': 'k', 'linewidth': 1.5}\n",
    "        \n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops, 'medianprops': lineprops,\n",
    "            'whiskerprops': lineprops, 'capprops': lineprops,\n",
    "            'width': box_width, 'palette': self.sensor_box_colors,\n",
    "            'hue_order': self.sensor_names\n",
    "        }\n",
    "\n",
    "        # Stripplot properties\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.1, 'size': strip_size, 'alpha': 0.3,\n",
    "            'palette': self.sensor_strip_colors, 'hue_order': self.sensor_names\n",
    "        }\n",
    "        \n",
    "        # Plotting with specified figure size and resolution\n",
    "        plt.figure(figsize=fig_size, dpi=dpi)\n",
    "        ax = plt.subplot()\n",
    "\n",
    "        sns.stripplot(\n",
    "            x='stimulation_id', y=df_column_name, hue='sensor_name',\n",
    "            data=self.combined_df, ax=ax, jitter=0.3, dodge=True,\n",
    "            **stripplot_kwargs\n",
    "        )\n",
    "        \n",
    "        ## error bars on the boxplot are the 95% confidence interval\n",
    "        sns.boxplot(\n",
    "            x='stimulation_id', y=df_column_name, hue='sensor_name',\n",
    "            data=self.combined_df, ax=ax, fliersize=0,\n",
    "            **boxplot_kwargs\n",
    "        )\n",
    "        \n",
    "        ### set the axis labels and ticks properties ### \n",
    "        ################################################\n",
    "            \n",
    "        # Set the font size of the x-axis and y-axis labels \n",
    "        ax.set_xlabel('Stimulation ID', fontsize=18)\n",
    "        ax.set_ylabel(df_column_name, fontsize=18)\n",
    "        \n",
    "        # Set the font size of the x-axis and y-axis ticks\n",
    "        ax.tick_params(axis='x', labelsize=24)\n",
    "        ax.tick_params(axis='y', labelsize=24)\n",
    "        ax.legend_.remove()\n",
    "        \n",
    "        #set the font to arial and the font size to 24\n",
    "        plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "        plt.tight_layout()  # Adjust layout to fit legend#\n",
    "        \n",
    "        \n",
    "        # Save the plot if a directory is specified\n",
    "        if save_dir is not None:\n",
    "            # Ensure the save directory exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Ensure that text is saved as text in the SVG\n",
    "            plt.rcParams['svg.fonttype'] = 'none'\n",
    "            \n",
    "            # Generate a filename that reflects the inputs\n",
    "            sensor_names = '_'.join(self.sensor_names)\n",
    "            stim_ids = '_'.join(map(str, selected_stim_ids))\n",
    "            filename = f'{save_dir}/{sensor_names}_{df_column_name}_stim_{stim_ids}.svg'\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(filename, format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "            print(f\"Figure saved as {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def plot_time_series(self, full_array_column, selected_stim_ids=None, fig_size=(6.5, 8), dpi=300,\n",
    "                        y_limits=None, save_dir=None, save_dpi=300, plot_sem=False, plot_sem_as_dotted=False):\n",
    "        if self.combined_df is None:\n",
    "            raise ValueError(\"Data has not been prepared for plotting. Call prepare_for_plotting first.\")\n",
    "\n",
    "        # Plot setup and layout\n",
    "        stim_ids = selected_stim_ids if selected_stim_ids is not None else self.combined_df['stimulation_id'].unique()\n",
    "        num_stim_ids = len(stim_ids)\n",
    "        num_sensors = len(self.sensor_names)\n",
    "\n",
    "        # Determine the maximum length of the time series data\n",
    "        max_sample_size = max(\n",
    "            self.combined_df[full_array_column].apply(lambda x: len(x))\n",
    "        )\n",
    "\n",
    "        # Create a common time vector based on the maximum sample size\n",
    "        frame_rate = 10  # Frames per second\n",
    "        time_vector = (np.arange(max_sample_size) - 10) / frame_rate  # Time in seconds, centered at 0\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(num_sensors, num_stim_ids,\n",
    "                                figsize=(fig_size[0] * num_stim_ids, fig_size[1] * num_sensors),\n",
    "                                dpi=dpi, sharey=True, sharex=True)\n",
    "\n",
    "        # Adjust axes to be a 2D array\n",
    "        if num_sensors == 1 and num_stim_ids == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif num_sensors == 1:\n",
    "            axes = np.array(axes).reshape(1, num_stim_ids)\n",
    "        elif num_stim_ids == 1:\n",
    "            axes = np.array(axes).reshape(num_sensors, 1)\n",
    "\n",
    "        for row_idx, sensor_name in enumerate(self.sensor_names):\n",
    "            for col_idx, stim_id in enumerate(stim_ids):\n",
    "                ax = axes[row_idx, col_idx]\n",
    "                sensor_stim_data = self.combined_df[\n",
    "                    (self.combined_df['sensor_name'] == sensor_name) &\n",
    "                    (self.combined_df['stimulation_id'] == stim_id)\n",
    "                ]\n",
    "\n",
    "                if not sensor_stim_data.empty:\n",
    "                    # Collect responses, padding with NaNs if necessary\n",
    "                    responses = []\n",
    "                    for _, row in sensor_stim_data.iterrows():\n",
    "                        data = row[full_array_column]\n",
    "                        if len(data) < max_sample_size:\n",
    "                            data = np.pad(data, (0, max_sample_size - len(data)), 'constant', constant_values=np.nan)\n",
    "                        responses.append(data)\n",
    "                    responses = np.array(responses)\n",
    "\n",
    "                    # Plot individual responses in grey (only if plot_sem is False)\n",
    "                    if not plot_sem:\n",
    "                        for response in responses:\n",
    "                            ax.plot(time_vector, response, color='gainsboro', alpha=0.3)\n",
    "\n",
    "                    # Plot median response\n",
    "                    median_response = np.nanmedian(responses, axis=0)\n",
    "                    ax.plot(time_vector, median_response, color=self.sensor_box_colors[sensor_name], label='Median')\n",
    "\n",
    "                    # Plot SEM as shaded region or dotted lines\n",
    "                    if plot_sem:\n",
    "                        sem_response = np.nanstd(responses, axis=0) / np.sqrt(np.sum(~np.isnan(responses), axis=0))\n",
    "                        if plot_sem_as_dotted:\n",
    "                            ax.plot(time_vector, median_response + sem_response, linestyle='--',\n",
    "                                    color=self.sensor_box_colors[sensor_name], linewidth=0.7)\n",
    "                            ax.plot(time_vector, median_response - sem_response, linestyle='--',\n",
    "                                    color=self.sensor_box_colors[sensor_name], linewidth=0.7)\n",
    "                        else:\n",
    "                            ax.fill_between(time_vector, median_response - sem_response,\n",
    "                                            median_response + sem_response,\n",
    "                                            color=self.sensor_box_colors[sensor_name], alpha=0.2, label='SEM')\n",
    "\n",
    "                    # Add horizontal line at y=0\n",
    "                    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "                    # Calculate the duration in seconds\n",
    "                    stim_duration = stim_id / 1000.0  # Convert ms to seconds\n",
    "\n",
    "                    # Add shaded region for stimulation window\n",
    "                    if stim_duration > 0:\n",
    "                        ax.axvspan(0, stim_duration, color='grey', alpha=0.1, zorder=1)\n",
    "                        # Add red dashed vertical lines on top of the shaded region\n",
    "                        ax.axvline(0, color='red', linestyle='--', linewidth=0.8, zorder=2)\n",
    "                        if stim_duration != 0.012:  # If there's a significant duration, add end line\n",
    "                            ax.axvline(stim_duration, color='red', linestyle='--', linewidth=0.8, zorder=2)\n",
    "                    else:\n",
    "                        ax.axvline(0, color='red', linestyle='--', linewidth=0.8, zorder=2)\n",
    "\n",
    "                else:\n",
    "                    # Hide axes with no data\n",
    "                    ax.axis('off')\n",
    "\n",
    "                # Set titles and labels\n",
    "                ax.set_title(f'Stim {stim_id} ms - {sensor_name}', fontsize=14)\n",
    "                ax.set_xlabel('Time (s)', fontsize=18)\n",
    "                if col_idx == 0:\n",
    "                    ax.set_ylabel('ΔF/F', fontsize=18)\n",
    "\n",
    "                if y_limits is not None:\n",
    "                    ax.set_ylim(y_limits)\n",
    "\n",
    "                ax.tick_params(axis='x', labelsize=14)\n",
    "                ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "        # Save the figure if a save directory is specified\n",
    "        if save_dir is not None:\n",
    "            sensor_name_str = '_'.join(self.sensor_names)  # Combine sensor names if multiple\n",
    "            fig.savefig(f'{save_dir}/{sensor_name_str}_plot_time_series.svg',\n",
    "                        format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def plot_single_roi(self, full_array_column, session_id, roi, selected_stim_ids=None, fig_size=(6.5, 8), dpi=300, y_limits=None, save_dir=None, save_dpi=300):\n",
    "        \"\"\"\n",
    "        Plots the time series data for a specific ROI within a given session for selected stimulation IDs.\n",
    "        :param full_array_column: The name of the column with time series data.\n",
    "        :param session_id: The ID of the session to filter.\n",
    "        :param roi: The specific ROI to plot.\n",
    "        :param selected_stim_ids: List of stimulation IDs to plot. If None, plot all available stimulations.\n",
    "        :param fig_size: Tuple representing the figure size of the plot in inches (default: 6.5 x 8).\n",
    "        :param dpi: The resolution in dots per inch for displaying the figure (default: 300).\n",
    "        :param y_limits: Tuple representing the y-axis min and max (y_min, y_max). If None, use default auto-scaling.\n",
    "        :param save_dir: Directory to save the plot. If None, the plot is not saved.\n",
    "        :param save_dpi: The resolution in dots per inch for saving the figure (default: 300).\n",
    "        \"\"\"\n",
    "        if self.combined_df is None:\n",
    "            raise ValueError(\"Data has not been prepared for plotting. Call prepare_for_plotting first.\")\n",
    "\n",
    "        # Debugging: Check if the session_id exists in the data\n",
    "        if session_id not in self.combined_df['session_id'].unique():\n",
    "            print(f\"Error: Session ID {session_id} does not exist in the dataset.\")\n",
    "            print(f\"Available session IDs: {self.combined_df['session_id'].unique()}\")\n",
    "            return\n",
    "\n",
    "        # Debugging: Check if the ROI exists within the given session\n",
    "        session_data = self.combined_df[self.combined_df['session_id'] == session_id]\n",
    "        if roi not in session_data['roi'].unique():\n",
    "            print(f\"Error: ROI {roi} does not exist for Session ID {session_id}.\")\n",
    "            print(f\"Available ROIs for session {session_id}: {session_data['roi'].unique()}\")\n",
    "            return\n",
    "\n",
    "        # Debugging statement: Confirmation that session and ROI exist\n",
    "        print(f\"Plotting data for Session ID: {session_id}, ROI: {roi}\")\n",
    "\n",
    "        # Filter the DataFrame for the specific session_id and ROI\n",
    "        filtered_data = session_data[session_data['roi'] == roi]\n",
    "\n",
    "        if filtered_data.empty:\n",
    "            raise ValueError(f\"No data found for session_id: {session_id} and roi: {roi}\")\n",
    "\n",
    "        # Filter by selected stimulation IDs or use all available if not provided\n",
    "        stim_ids = selected_stim_ids if selected_stim_ids is not None else filtered_data['stimulation_id'].unique()\n",
    "\n",
    "        # Create a plot for the selected ROI and stimulation IDs\n",
    "        fig, ax = plt.subplots(figsize=fig_size, dpi=dpi)\n",
    "\n",
    "        # Plot each selected stimulation ID\n",
    "        for stim_id in stim_ids:\n",
    "            stim_data = filtered_data[filtered_data['stimulation_id'] == stim_id]\n",
    "            if stim_data.empty:\n",
    "                print(f\"No data found for Stimulation ID: {stim_id}\")\n",
    "                continue\n",
    "            \n",
    "            row = stim_data.iloc[0]  # Get the first row for the given stim_id\n",
    "            \n",
    "            # Time series plotting\n",
    "            sample_size = len(row[full_array_column])\n",
    "            time_vector = (np.arange(sample_size) - 10) * 100  # Adjust time_vector for 100 ms intervals\n",
    "\n",
    "            # Plot the time series for the selected ROI and stim_id\n",
    "            ax.plot(time_vector, row[full_array_column], label=f'Stim ID: {stim_id}')\n",
    "\n",
    "        ax.set_title(f'Session {session_id} - ROI {roi}', fontsize=14)\n",
    "        ax.set_xlabel('Time (ms)', fontsize=18)\n",
    "        ax.set_ylabel('ΔF/F', fontsize=18)\n",
    "\n",
    "        # Set y-axis limits if provided\n",
    "        if y_limits is not None:\n",
    "            ax.set_ylim(y_limits)\n",
    "\n",
    "        ax.tick_params(axis='x', labelsize=24)\n",
    "        ax.tick_params(axis='y', labelsize=24)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "        # Save the plot if a directory is specified\n",
    "        if save_dir is not None:\n",
    "            # Ensure that text is saved as text in the SVG by setting the 'text.usetex' option to False\n",
    "            mpl.rcParams['svg.fonttype'] = 'none'  # Ensures text is saved as editable text, not paths\n",
    "            fig.savefig(f'{save_dir}/session_{session_id}_ROI_{roi}.svg', format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "\n",
    "    def plot_non_responsive_heatmap_and_pie(self, selected_stim_id, fig_size=(15, 6), dpi=300, \n",
    "                                            cmap='rocket', vmin=0, vmax=None, smooth_method=None, \n",
    "                                            smooth_sigma=1, save_dir=None, save_dpi=300,\n",
    "                                            interpolation='gaussian'):\n",
    "        \"\"\"\n",
    "        Creates a 1x2 panel with a heatmap of traces and a pie chart of responsive vs. non-responsive ROIs.\n",
    "        \n",
    "        :param selected_stim_id: The stimulation ID to analyze.\n",
    "        :param fig_size: Tuple representing the figure size (width, height) in inches.\n",
    "        :param dpi: The resolution in dots per inch for display.\n",
    "        :param cmap: Colormap to use. Default is 'rocket'.\n",
    "        :param vmin: Minimum value for colormap scaling. Default is 0.\n",
    "        :param vmax: Maximum value for colormap scaling. If None, it's set to the data maximum.\n",
    "        :param smooth_method: Method for smoothing. Options are 'gaussian', 'moving_average', or None. Default is None (no smoothing).\n",
    "        :param smooth_sigma: Sigma for Gaussian filter or window size for moving average. Default is 1.\n",
    "        :param save_dir: Directory to save the plot. If None, the plot is not saved.\n",
    "        :param save_dpi: The resolution in dots per inch for saving the figure (default: 300).\n",
    "        :param interpolation: Interpolation method for imshow. Options include 'nearest', 'bilinear', 'bicubic', 'spline16',\n",
    "                          'spline36', 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric', 'catrom', 'gaussian', \n",
    "                          'bessel', 'mitchell', 'sinc', 'lanczos'. Default is 'gaussian'.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Access the dataframe\n",
    "        df = self.data_frames[0]\n",
    "        \n",
    "        # Filter data for the selected stimulation ID\n",
    "        stim_data = df[df['stimulation_id'] == selected_stim_id]\n",
    "        \n",
    "        # Process delta_f_f_full_array data\n",
    "        def process_array(arr):\n",
    "            arr = np.array(arr)\n",
    "            arr[np.isnan(arr) | (arr < 0)] = 0\n",
    "            return arr\n",
    "\n",
    "        stim_data['processed_array'] = stim_data['delta_f_f_full_array'].apply(process_array)\n",
    "        \n",
    "        # Separate responsive and non-responsive data\n",
    "        responsive_data = stim_data[stim_data['is_responsive']]\n",
    "        non_responsive_data = stim_data[~stim_data['is_responsive']]\n",
    "        \n",
    "        # Rank responsive neurons (lowest to highest)\n",
    "        responsive_data['max_delta_f'] = responsive_data['processed_array'].apply(np.max)\n",
    "        responsive_data = responsive_data.sort_values('max_delta_f', ascending=True)\n",
    "\n",
    "        # Combine data\n",
    "        combined_data = pd.concat([responsive_data, non_responsive_data])\n",
    "        \n",
    "        #smoothing function\n",
    "        def smooth_data(data, method=None, sigma=1):\n",
    "            if method == 'gaussian':\n",
    "                return gaussian_filter1d(data, sigma=sigma)\n",
    "            elif method == 'moving_average':\n",
    "                window = np.ones(int(sigma)) / float(sigma)\n",
    "                return np.convolve(data, window, 'same')\n",
    "            else:\n",
    "                return data  # No smoothing\n",
    "\n",
    "        # Apply smoothing to the processed arrays (or not, if smooth_method is None)\n",
    "        if smooth_method is not None:\n",
    "            combined_data['plot_array'] = combined_data['processed_array'].apply(\n",
    "                lambda x: smooth_data(x, method=smooth_method, sigma=smooth_sigma)\n",
    "            )\n",
    "        else:\n",
    "            combined_data['plot_array'] = combined_data['processed_array']\n",
    "        \n",
    "        \n",
    "        # Create the figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size, dpi=dpi)\n",
    "        \n",
    "        # Heatmap\n",
    "        heatmap_data = np.vstack(combined_data['processed_array'].values)\n",
    "        \n",
    "        # Calculate the maximum value for color scaling if not provided\n",
    "        if vmax is None:\n",
    "            vmax = np.nanmax(heatmap_data)\n",
    "        \n",
    "        # Create a custom colormap that ensures zero values are black\n",
    "        base_cmap = plt.get_cmap(cmap)\n",
    "        colors = base_cmap(np.linspace(0, 1, 256))\n",
    "        colors[0] = [0, 0, 0, 1]  # Set the first color to black\n",
    "        custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
    "        \n",
    "        # Plot the heatmap (older approach using sns.heatmap)\n",
    "        #sns.heatmap(heatmap_data, ax=ax1, cmap=custom_cmap, vmin=vmin, vmax=vmax, \n",
    "        #            cbar_kws={'label': 'dF/F', 'extend': 'max'})\n",
    "        \n",
    "        # Plot the heatmap using imshow\n",
    "        im = ax1.imshow(heatmap_data, cmap=custom_cmap, aspect='auto', interpolation=interpolation,\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "        # Add colorbar\n",
    "        fig.colorbar(im, ax=ax1, label='dF/F')\n",
    "\n",
    "        \n",
    "        ax1.set_title(f'Neuron Traces (Stim ID: {selected_stim_id})')\n",
    "        ax1.set_ylabel('Neuron #')\n",
    "        \n",
    "        # Adjust y-axis ticks\n",
    "        num_neurons = len(combined_data)\n",
    "        ax1.set_yticks([0, num_neurons-1])\n",
    "        ax1.set_yticklabels([1, num_neurons])\n",
    "        \n",
    "        # Add a vertical line at stimulus onset\n",
    "        ax1.axvline(x=10, color='white', linestyle='--', linewidth=2)\n",
    "        \n",
    "        # Adjust x-axis to show time relative to stimulus onset\n",
    "        num_frames = heatmap_data.shape[1]\n",
    "        time_points = np.linspace(-1, (num_frames-11)/10, num_frames)\n",
    "        ax1.set_xticks(np.linspace(0, num_frames-1, 5))\n",
    "        ax1.set_xticklabels([f'{t:.1f}' for t in np.linspace(time_points[0], time_points[-1], 5)])\n",
    "\n",
    "        ax1.set_xlabel('Time relative to stimulus onset (s)')\n",
    "\n",
    "        \n",
    "        # Add dotted line to separate responsive and non-responsive neurons\n",
    "        responsive_count = len(responsive_data)\n",
    "        ax1.axhline(y=responsive_count, color='white', linestyle=':', linewidth=2)\n",
    "        \n",
    "        # Pie chart\n",
    "        sizes = [len(responsive_data), len(non_responsive_data)]\n",
    "        labels = ['Responsive', 'Non-responsive']\n",
    "        colors = ['#ccccff', '#9933ff']  # Light blue and purple\n",
    "        \n",
    "        ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title(f'ROI Responsiveness (Stim ID: {selected_stim_id})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "            # Save the plot if a directory is specified\n",
    "        if save_dir is not None:\n",
    "            # Ensure the save directory exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Ensure that text is saved as text in the SVG\n",
    "            plt.rcParams['svg.fonttype'] = 'none'  # Ensures text is saved as editable text, not paths\n",
    "            \n",
    "            # Get the sensor name (assuming the first sensor is the relevant one)\n",
    "            sensor_name = self.sensor_names[0] if self.sensor_names else \"unknown_sensor\"\n",
    "            \n",
    "            # Generate the filename\n",
    "            filename = f'{save_dir}/{sensor_name}_heatmap_pie_stim_{selected_stim_id}.svg'\n",
    "            \n",
    "            # Save the figure\n",
    "            fig.savefig(filename, format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "            print(f\"Figure saved as {filename}\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_cumulative_distribution(self, column_name='time_to_peak', selected_stim_ids=None, fig_size=(10, 6), dpi=300, save_dir=None, save_dpi=300, num_points=1000):\n",
    "        \"\"\"\n",
    "        Plots the cumulative distribution of a specified column (default: time_to_peak) for each sensor.\n",
    "        \n",
    "        :param column_name: The name of the column to plot (default: 'time_to_peak')\n",
    "        :param selected_stim_ids: List of stimulation IDs to include. If None, use all.\n",
    "        :param fig_size: Tuple representing the figure size (width, height) in inches.\n",
    "        :param dpi: The resolution in dots per inch for display.\n",
    "        :param save_dir: Directory to save the plot. If None, the plot is not saved.\n",
    "        :param save_dpi: The resolution in dots per inch for saving the figure.\n",
    "        :param num_points: Number of points to use for the cumulative distribution (default: 1000)\n",
    "        \"\"\"\n",
    "        if self.combined_df is None:\n",
    "            self.prepare_for_plotting(column_name)\n",
    "        \n",
    "        # Filter data for selected stimulation IDs if provided\n",
    "        if selected_stim_ids is not None:\n",
    "            plot_df = self.combined_df[self.combined_df['stimulation_id'].isin(selected_stim_ids)]\n",
    "        else:\n",
    "            plot_df = self.combined_df\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=fig_size, dpi=dpi)\n",
    "        \n",
    "        for sensor_name in self.sensor_names:\n",
    "            sensor_data = plot_df[plot_df['sensor_name'] == sensor_name][column_name]\n",
    "            sorted_data = np.sort(sensor_data)\n",
    "            \n",
    "            # Create evenly spaced points for the x-axis\n",
    "            x = np.linspace(sorted_data.min(), sorted_data.max(), num_points)\n",
    "            \n",
    "            # Calculate the cumulative distribution\n",
    "            y = np.zeros_like(x)\n",
    "            for i, value in enumerate(x):\n",
    "                y[i] = np.sum(sorted_data <= value) / len(sorted_data) * 100\n",
    "\n",
    "            # Add vertical line at the start\n",
    "            plt.vlines(x[0], 0, y[0], color=self.sensor_box_colors[sensor_name], linestyle='-')\n",
    "            \n",
    "            # Plot the cumulative distribution\n",
    "            plt.plot(x, y, label=sensor_name, color=self.sensor_box_colors[sensor_name])\n",
    "\n",
    "        plt.xlabel(column_name, fontsize=14)\n",
    "        plt.ylabel('Cumulative Percentage', fontsize=14)\n",
    "        plt.title(f'Cumulative Distribution of {column_name}', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Remove grid\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # Set tick label font size\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot if a directory is specified\n",
    "        if save_dir is not None:\n",
    "            # Ensure the save directory exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # Ensure that text is saved as text in the SVG\n",
    "            plt.rcParams['svg.fonttype'] = 'none'\n",
    "            \n",
    "            # Generate a filename that reflects the inputs\n",
    "            sensor_names = '_'.join(self.sensor_names)\n",
    "            stim_ids = '_'.join(map(str, selected_stim_ids)) if selected_stim_ids else 'all'\n",
    "            filename = f'{save_dir}/{sensor_names}_{column_name}_cumulative_dist_stim_{stim_ids}.svg'\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(filename, format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "            print(f\"Figure saved as {filename}\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def run_ks_test(self, column_name='time_to_peak', selected_stim_ids=None):\n",
    "        \"\"\"\n",
    "        Runs a two-tailed Kolmogorov-Smirnov test on the specified column for each pair of sensors.\n",
    "        \n",
    "        :param column_name: The name of the column to analyze (default: 'time_to_peak')\n",
    "        :param selected_stim_ids: List of stimulation IDs to include. If None, use all.\n",
    "        :return: DataFrame with test results\n",
    "        \"\"\"\n",
    "        if self.combined_df is None:\n",
    "            self.prepare_for_plotting(column_name)\n",
    "        \n",
    "        # Filter data for selected stimulation IDs if provided\n",
    "        if selected_stim_ids is not None:\n",
    "            plot_df = self.combined_df[self.combined_df['stimulation_id'].isin(selected_stim_ids)]\n",
    "        else:\n",
    "            plot_df = self.combined_df\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Perform KS test for each pair of sensors\n",
    "        for i in range(len(self.sensor_names)):\n",
    "            for j in range(i+1, len(self.sensor_names)):\n",
    "                sensor1 = self.sensor_names[i]\n",
    "                sensor2 = self.sensor_names[j]\n",
    "\n",
    "                data1 = plot_df[plot_df['sensor_name'] == sensor1][column_name]\n",
    "                data2 = plot_df[plot_df['sensor_name'] == sensor2][column_name]\n",
    "\n",
    "                # Perform two-tailed KS test\n",
    "                ks_statistic, p_value = stats.ks_2samp(data1, data2)\n",
    "\n",
    "                results.append({\n",
    "                    'Sensor1': sensor1,\n",
    "                    'Sensor2': sensor2,\n",
    "                    'N1': len(data1),\n",
    "                    'N2': len(data2),\n",
    "                    'KS_statistic': ks_statistic,\n",
    "                    'p_value': p_value,\n",
    "                    'Column': column_name,\n",
    "                    'Stim_IDs': ', '.join(map(str, selected_stim_ids)) if selected_stim_ids else 'All'\n",
    "                })\n",
    "\n",
    "        # Create DataFrame from results\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Add significance stars\n",
    "        def get_significance(p):\n",
    "            if p < 0.001:\n",
    "                return '***'\n",
    "            elif p < 0.01:\n",
    "                return '**'\n",
    "            elif p < 0.05:\n",
    "                return '*'\n",
    "            else:\n",
    "                return 'ns'\n",
    "\n",
    "        results_df['Significance'] = results_df['p_value'].apply(get_significance)\n",
    "\n",
    "        # Format p-value\n",
    "        results_df['p_value_formatted'] = results_df['p_value'].apply(lambda p: f\"{p:.4f}\" if p >= 0.0001 else \"<0.0001\")\n",
    "\n",
    "        # Reorder columns to put p-value next to KS_statistic\n",
    "        column_order = ['Sensor1', 'Sensor2', 'N1', 'N2', 'KS_statistic', 'p_value', 'p_value_formatted', 'Significance', 'Column', 'Stim_IDs']\n",
    "        results_df = results_df[column_order]\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def plot_mean_with_error(self, df_column_name, error_type='SEM', selected_stim_ids=None, xlim=None, ylim=None, fig_size=(8, 6), dpi=300, save_dir=None, save_dpi=300):\n",
    "        \"\"\"\n",
    "        Plots the mean values with error bars for selected stimulation IDs across sensors, using consistent colors and a logarithmic x-axis.\n",
    "        :param df_column_name: The name of the column to use for the value in the plot.\n",
    "        :param error_type: The type of error to display ('SD' for Standard Deviation or 'SEM' for Standard Error of the Mean).\n",
    "        :param selected_stim_ids: List of stimulation IDs to plot. If None, plot all.\n",
    "        :param xlim: Tuple representing the x-axis limits (min, max). If None, use default.\n",
    "        :param ylim: Tuple representing the y-axis limits (min, max). If None, use default.\n",
    "        :param fig_size: Tuple representing the figure size (width, height) in inches.\n",
    "        :param dpi: The resolution in dots per inch.\n",
    "        :param save_dir: Directory to save the plot. If None, the plot is not saved.\n",
    "        :param save_dpi: The resolution in dots per inch for saving the figure.\n",
    "        \"\"\"\n",
    "        if self.combined_df is None:\n",
    "            self.prepare_for_plotting(df_column_name)\n",
    "        \n",
    "        # Filter for selected stimulation IDs if provided\n",
    "        if selected_stim_ids is not None:\n",
    "            plot_df = self.combined_df[self.combined_df['stimulation_id'].isin(selected_stim_ids)]\n",
    "        else:\n",
    "            plot_df = self.combined_df\n",
    "            selected_stim_ids = sorted(plot_df['stimulation_id'].unique())\n",
    "\n",
    "        plt.figure(figsize=fig_size, dpi=dpi)\n",
    "        ax = plt.subplot()\n",
    "        \n",
    "        # Plot the mean with error bars for each sensor\n",
    "        for sensor_name in self.sensor_names:\n",
    "            sensor_data = plot_df[plot_df['sensor_name'] == sensor_name]\n",
    "            means = sensor_data.groupby('stimulation_id')[df_column_name].mean()\n",
    "            if error_type == 'SD':\n",
    "                errors = sensor_data.groupby('stimulation_id')[df_column_name].std()\n",
    "            else:  # SEM\n",
    "                errors = sensor_data.groupby('stimulation_id')[df_column_name].sem()\n",
    "            \n",
    "            ax.errorbar(means.index, means, yerr=errors, label=sensor_name,\n",
    "                        fmt='-o', capsize=5, color=self.sensor_box_colors[sensor_name])\n",
    "\n",
    "        # Set x-axis to logarithmic scale\n",
    "        ax.set_xscale('log')\n",
    "        \n",
    "        # Set custom x-ticks to show all stimulation IDs\n",
    "        ax.set_xticks(selected_stim_ids)\n",
    "        ax.set_xticklabels(selected_stim_ids)\n",
    "        \n",
    "        # Adjust x-axis limits if not provided\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        else:\n",
    "            ax.set_xlim(min(selected_stim_ids) * 0.8, max(selected_stim_ids) * 1.2)\n",
    "        \n",
    "        # Set custom y-axis limits if provided\n",
    "        if ylim:\n",
    "            ax.set_ylim(ylim)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Stimulation ID', fontsize=14)\n",
    "        ax.set_ylabel(df_column_name, fontsize=14)\n",
    "        ax.set_title(f'Mean {df_column_name} by Stimulation ID across Sensors', fontsize=16)\n",
    "        \n",
    "        # Customize tick label sizes\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend(title='Sensor', loc='best', fontsize=10)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        ax.grid(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot if a directory is specified\n",
    "        if save_dir is not None:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            plt.rcParams['svg.fonttype'] = 'none'\n",
    "            sensor_names = '_'.join(self.sensor_names)\n",
    "            stim_ids = '_'.join(map(str, selected_stim_ids))\n",
    "            filename = f'{save_dir}/{sensor_names}_{df_column_name}_mean_error_stim_{stim_ids}.svg'\n",
    "            plt.savefig(filename, format='svg', dpi=save_dpi, transparent=True, bbox_inches='tight')\n",
    "            print(f\"Figure saved as {filename}\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/Volumes/MannySSD/cablam_imaging/raw_data_template_copy/data' #path to the folder containing the raw data to be analyzed (i.e. the folder containing the folders for each experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 --preparing all the data and directories. Do this once with all the data \n",
    "### run this section of code before running the cablam and gcamp8 code instances to avoid errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = ImageAnalysis(project_folder) #initialize the ImageAnalysis class with the project folder\n",
    "analysis.expand_directory_df() #expand the directory DataFrame to include sensor type, session ID, stimulation IDs, and stimulation frame numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.add_tiff_dimensions() #add the dimensions of the tif files to the directory DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.directory_df #display the updated directory DataFrame to confirm the changes of adding the tiff dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this will create the directories and save the max projection images for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analysis.analyze_all_sessions(analysis.analyze_session_max_projection)\n",
    "\n",
    "# Output the results variable to confirm the max projection images were saved\n",
    "for session_id, result_path in results.items():\n",
    "    if isinstance(result_path, str):\n",
    "        print(f\"Session ID {session_id}: Max projection image saved at {result_path}\")\n",
    "    else:\n",
    "        print(f\"Session ID {session_id}: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point add your manual ROIs .tif file of handdrawn ROIs to the corresponding 'processed_image_analysis_output' directory. \n",
    "\n",
    "Drag and drop the 'labels_postexport.tif' file into the 'processed_image_analysis_output' directory that was created in the previous step to the matching sensor found in 'manual_mask_for_cablam' and 'manual_mask_for_gcamp' folders. Ensure the .tif files have the same matching xy pixel size to ensure the correct aligmen to the .tif calcium movies. \n",
    "\n",
    "The code assumes the manual/ML generated masks are called 'labels_postexport.tif' and extracts this files itteratively for every unique session ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract calcium signals for all sessions in the directory_df, confirms the location of the saved calcium signals and the location of the saved calcium signals\n",
    "all_results = analysis.analyze_all_calcium_signals()\n",
    "\n",
    "for session_id, csv_path in all_results.items():\n",
    "    if isinstance(csv_path, str):\n",
    "        print(f\"Session {session_id} - Calcium signals saved at: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"Session {session_id} - Error: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Analyzing Image Data for Multiple Sensor Types. \n",
    "### Section 2a: once the ImageAnalysis class has been initialized at least once for all files and directory DataFrame has been expanded to include the sensor type, session ID, stimulation IDs, and stimulation frame numbers, we can filter the directory DataFrame to analyze image data for specific sensor types. In this example, we will analyze image data for two sensor types: gcamp8 and cablam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory_name</th>\n",
       "      <th>directory_path</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>stimulation_ids</th>\n",
       "      <th>stimulation_frame_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c21_12232023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2112232023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3509, 3911, 4313, 4715, 5118, 5521, 5926, 633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c11_12242023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>1112242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3597, 3999, 4402, 4804, 5207, 5610, 6014, 642...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c12_12232023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>1212232023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 480]</td>\n",
       "      <td>[3587, 3788, 3990, 4191, 4393, 4595]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c12_12242023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>1212242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3589, 3991, 4393, 4796, 5198, 5602, 6006, 641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c13_12242023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>1312242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3585, 3987, 4390, 4792, 5195, 5598, 6002, 640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c22_12232023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2212232023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3583, 3985, 4387, 4789, 5192, 5595, 6000, 640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c23_12232023_estim_10hz_1xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2312232023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3592, 3994, 4396, 4799, 5202, 5605, 6009, 641...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  directory_name  \\\n",
       "1   c21_12232023_estim_10hz_1xfz   \n",
       "2   c11_12242023_estim_10hz_1xfz   \n",
       "4   c12_12232023_estim_10hz_1xfz   \n",
       "6   c12_12242023_estim_10hz_1xfz   \n",
       "8   c13_12242023_estim_10hz_1xfz   \n",
       "10  c22_12232023_estim_10hz_1xfz   \n",
       "11  c23_12232023_estim_10hz_1xfz   \n",
       "\n",
       "                                       directory_path sensor_type  session_id  \\\n",
       "1   /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2112232023   \n",
       "2   /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  1112242023   \n",
       "4   /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  1212232023   \n",
       "6   /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  1212242023   \n",
       "8   /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  1312242023   \n",
       "10  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2212232023   \n",
       "11  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2312232023   \n",
       "\n",
       "                               stimulation_ids  \\\n",
       "1   [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "2   [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "4                   [12, 24, 36, 60, 120, 480]   \n",
       "6   [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "8   [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "10  [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "11  [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "\n",
       "                             stimulation_frame_number  \n",
       "1   [3509, 3911, 4313, 4715, 5118, 5521, 5926, 633...  \n",
       "2   [3597, 3999, 4402, 4804, 5207, 5610, 6014, 642...  \n",
       "4                [3587, 3788, 3990, 4191, 4393, 4595]  \n",
       "6   [3589, 3991, 4393, 4796, 5198, 5602, 6006, 641...  \n",
       "8   [3585, 3987, 4390, 4792, 5195, 5598, 6002, 640...  \n",
       "10  [3583, 3985, 4387, 4789, 5192, 5595, 6000, 640...  \n",
       "11  [3592, 3994, 4396, 4799, 5202, 5605, 6009, 641...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of the ImageAnalysis class for each sensor type: gcamp8 and cablam\n",
    "### cablam analysis ###\n",
    "analysis_cablam1x = ImageAnalysis(project_folder)\n",
    "analysis_cablam1x.expand_directory_df() #expand the directory_df to include the sensor_type, session_id, stimulation_ids, and stimulation_frame_number\n",
    "#filter the directory_df to only include the rows where the sensor_type is 'cablam' and the directory_name contains '1xfz'\n",
    "analysis_cablam1x.directory_df = analysis_cablam1x.directory_df[(analysis_cablam1x.directory_df['sensor_type'] == 'cablam') & (analysis_cablam1x.directory_df['directory_name'].str.contains('1xfz'))]\n",
    "analysis_cablam1x.directory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory_name</th>\n",
       "      <th>directory_path</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>stimulation_ids</th>\n",
       "      <th>stimulation_frame_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g12_12092023_estim_10hz_na_blk</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>gcamp8</td>\n",
       "      <td>1212092023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 480]</td>\n",
       "      <td>[3579, 3780, 3982, 4183, 4385, 4587]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g13_12092023_estim_10hz_na_blk</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>gcamp8</td>\n",
       "      <td>1312092023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 480]</td>\n",
       "      <td>[3599, 3800, 4001, 4203, 4404, 4607]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   directory_name  \\\n",
       "0  g12_12092023_estim_10hz_na_blk   \n",
       "9  g13_12092023_estim_10hz_na_blk   \n",
       "\n",
       "                                      directory_path sensor_type  session_id  \\\n",
       "0  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      gcamp8  1212092023   \n",
       "9  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      gcamp8  1312092023   \n",
       "\n",
       "              stimulation_ids              stimulation_frame_number  \n",
       "0  [12, 24, 36, 60, 120, 480]  [3579, 3780, 3982, 4183, 4385, 4587]  \n",
       "9  [12, 24, 36, 60, 120, 480]  [3599, 3800, 4001, 4203, 4404, 4607]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### gcamp8 analysis ###\n",
    "analysis_gcamp8 = ImageAnalysis(project_folder)\n",
    "analysis_gcamp8.expand_directory_df() #expand the directory_df to include the sensor_type, session_id, stimulation_ids, and stimulation_frame_number\n",
    "#filter the directory_df to only include the rows where the sensor_type is 'gcamp8'\n",
    "analysis_gcamp8.directory_df = analysis_gcamp8.directory_df[(analysis_gcamp8.directory_df['sensor_type'] == 'gcamp8')]\n",
    "analysis_gcamp8.directory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>directory_name</th>\n",
       "      <th>directory_path</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>stimulation_ids</th>\n",
       "      <th>stimulation_frame_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c21_12242023_estim_10hz_05xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2112242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3592, 3994, 4397, 4799, 5202, 5605, 6009, 641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c22_12242023_estim_10hz_05xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2212242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3589, 3991, 4394, 4796, 5199, 5602, 6006, 641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c23_12242023_estim_10hz_05xfz</td>\n",
       "      <td>/Volumes/MannySSD/cablam_imaging/raw_data_temp...</td>\n",
       "      <td>cablam</td>\n",
       "      <td>2312242023</td>\n",
       "      <td>[12, 24, 36, 60, 120, 240, 480, 960, 1920]</td>\n",
       "      <td>[3589, 3991, 4394, 4796, 5199, 5602, 6006, 641...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  directory_name  \\\n",
       "3  c21_12242023_estim_10hz_05xfz   \n",
       "5  c22_12242023_estim_10hz_05xfz   \n",
       "7  c23_12242023_estim_10hz_05xfz   \n",
       "\n",
       "                                      directory_path sensor_type  session_id  \\\n",
       "3  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2112242023   \n",
       "5  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2212242023   \n",
       "7  /Volumes/MannySSD/cablam_imaging/raw_data_temp...      cablam  2312242023   \n",
       "\n",
       "                              stimulation_ids  \\\n",
       "3  [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "5  [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "7  [12, 24, 36, 60, 120, 240, 480, 960, 1920]   \n",
       "\n",
       "                            stimulation_frame_number  \n",
       "3  [3592, 3994, 4397, 4799, 5202, 5605, 6009, 641...  \n",
       "5  [3589, 3991, 4394, 4796, 5199, 5602, 6006, 641...  \n",
       "7  [3589, 3991, 4394, 4796, 5199, 5602, 6006, 641...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### cablam 05x fz analysis ###\n",
    "\n",
    "#filter fpr the 05xFz condition \n",
    "analysis_cablam05x = ImageAnalysis(project_folder)\n",
    "analysis_cablam05x.expand_directory_df() #expand the directory_df to include the sensor_type, session_id, stimulation_ids, and stimulation_frame_number\n",
    "#filter the directory_df to only include the rows where the sensor_type is 'cablam' and the directory_name contains '05xfz'\n",
    "analysis_cablam05x.directory_df = analysis_cablam05x.directory_df[(analysis_cablam05x.directory_df['sensor_type'] == 'cablam') & (analysis_cablam05x.directory_df['directory_name'].str.contains('05xfz'))]\n",
    "analysis_cablam05x.directory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2b.1: you must use corrected_data as False the first run through if this is the first time running the process_all_sessions method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_gcamp8 = analysis_gcamp8.process_all_sessions(use_corrected_data=False)\n",
    "all_data_cablam1x = analysis_cablam1x.process_all_sessions(use_corrected_data=False)\n",
    "all_data_cablam05x = analysis_cablam05x.process_all_sessions(use_corrected_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### let see what the all_data_gcamp8 looks likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session_id, session_data in all_data_gcamp8.items():\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    print(f\"Stimulation Frame Numbers: {session_data['stim_frame_numbers']}\")\n",
    "    print(f\"Stimulation IDs: {session_data['stimulation_ids']}\")\n",
    "    for roi, roi_data in session_data['roi_data'].items():\n",
    "        print(f\"ROI: {roi}\")\n",
    "        for key, value in roi_data.items():\n",
    "            print(f\"Stimulation ID, Frame Number tuple: {key}\")\n",
    "            print(f\"Data: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2b.2: once you ran process_all_sessions once, you can now implement the median substraction method create a corrected CSV files via the process_biolumi_calcium_signal method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session ID: 2112232023\n",
      "Completed processing for session ID: 2112232023\n",
      "Processing session ID: 1112242023\n",
      "Completed processing for session ID: 1112242023\n",
      "Processing session ID: 1212232023\n",
      "Completed processing for session ID: 1212232023\n",
      "Processing session ID: 1212242023\n",
      "Completed processing for session ID: 1212242023\n",
      "Processing session ID: 1312242023\n",
      "Completed processing for session ID: 1312242023\n",
      "Processing session ID: 2212232023\n",
      "Completed processing for session ID: 2212232023\n",
      "Processing session ID: 2312232023\n",
      "Completed processing for session ID: 2312232023\n",
      "Processing session ID: 2112242023\n",
      "Completed processing for session ID: 2112242023\n",
      "Processing session ID: 2212242023\n",
      "Completed processing for session ID: 2212242023\n",
      "Processing session ID: 2312242023\n",
      "Completed processing for session ID: 2312242023\n"
     ]
    }
   ],
   "source": [
    "analysis_cablam1x.process_all_sessions_biolumi() #now create the corrected calcium signals for the cablam1x data and save the csv files\n",
    "analysis_cablam05x.process_all_sessions_biolumi() #now create the corrected calcium signals for the cablam05x data and save the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2b.3: now you can create the data needed for plotting and downstream analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cablam1x = analysis_cablam1x.process_all_sessions(use_corrected_data=True) # reimport the data with the corrected calcium signals if the corrected data is True \n",
    "all_data_cablam1x_session_data = analysis_cablam1x.process_all_sessions_entire_recording(use_corrected_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cablam05x = analysis_cablam05x.process_all_sessions(use_corrected_data=True) # reimport the data with the corrected calcium signals if the corrected data is True\n",
    "all_data_cablam05x_session_data = analysis_cablam05x.process_all_sessions_entire_recording(use_corrected_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1212092023: Number of columns before operation: 19\n",
      "Some ROIs not found in session 1212092023: ['ROI_11', 'ROI_12', 'ROI_13']\n",
      "Session 1312092023: Number of columns before operation: 18\n",
      "Some ROIs not found in session 1312092023: ['ROI_2']\n"
     ]
    }
   ],
   "source": [
    "## now accoutns for the background correction for the gcamp8 data but still must run these two lines of code to process the data in the first place, must change for redundancy\n",
    "#all_data_gcamp8_session_data = analysis_gcamp8.process_all_sessions_entire_recording(use_corrected_data=False) #always keep tihs false because the gcamp8 data has not been corrected\n",
    "\n",
    "#  session IDs and corresponding ROIs for those sessions to be removed manually\n",
    "sessions = ['1212092023', '1312092023']  # This could be more than one session\n",
    "rois_list = [['ROI_11', 'ROI_12', 'ROI_13'], ['ROI_2']]  # List of ROIs for each session\n",
    "\n",
    "all_data_gcamp8_session_data = analysis_gcamp8.process_all_sessions_entire_recording_gcampbackgroundcorrected(sessions, rois_list, use_corrected_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now accoutns for the background correction \n",
    "\n",
    "#  session IDs and corresponding ROIs for those sessions to be removed manually\n",
    "sessions_cablam05x = ['2112242023', '2212242023', '2312242023']\n",
    "rois_list_cablam05x = [['ROI_25'], ['ROI_42'], ['ROI_47']]\n",
    "all_data_cablam05x_session_data = analysis_cablam05x.process_all_sessions_entire_recording_gcampbackgroundcorrected(sessions_cablam05x, rois_list_cablam05x, use_corrected_data=False)\n",
    "\n",
    "sessions_cablam1x = ['1112242023', '1212232023','1212242023', '1312242023', '2112232023', '2212232023', '2312232023' ]  # This could be more than one session\n",
    "rois_list_cablam1x = [['ROI_11'], ['ROI_4'], ['ROI_17'], ['ROI_4'], ['ROI_24'], ['ROI_27'], ['ROI_41']]  # List of ROIs for each session\n",
    "all_data_cablam1x_session_data = analysis_cablam1x.process_all_sessions_entire_recording_gcampbackgroundcorrected(sessions_cablam1x, rois_list_cablam1x, use_corrected_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Calculate responsiveness to generate the dictionary and dfs for each sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3a - Define responsive ROIs first and create necessary variables: the output of calculate_responsiveness are a the data in dictionary or dataframe format which has all the session ID in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsiveness_data_gcamp8, responsiveness_df_gcamp8 = analysis_gcamp8.calculate_responsiveness(all_data_gcamp8, return_dataframe=True)\n",
    "responsiveness_data_cablam1x, responsiveness_df_cablam1x = analysis_cablam1x.calculate_responsiveness(all_data_cablam1x, return_dataframe=True)\n",
    "respoinsiveness_data_cablam05x, responsiveness_df_cablam05x = analysis_cablam05x.calculate_responsiveness(all_data_cablam05x, return_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3b.1 - filter data to remove non responive neurons using the dictionary as the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_gcamp8 = analysis_gcamp8.filter_responsive_rois(all_data_gcamp8, responsiveness_data_gcamp8)\n",
    "filtered_data_cablam1x = analysis_cablam1x.filter_responsive_rois(all_data_cablam1x, responsiveness_data_cablam1x)\n",
    "filtered_data_cablam05x = analysis_cablam05x.filter_responsive_rois(all_data_cablam05x, respoinsiveness_data_cablam05x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3b.2 - filter data to remove non responive neurons using the dataframe as the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablam_filtered_responsive_rois = analysis_cablam1x.filter_responsive_rois_by_stimulation(all_data_cablam1x_session_data, responsiveness_df_cablam1x)\n",
    "cablam_filtered_responsive_rois05x = analysis_cablam05x.filter_responsive_rois_by_stimulation(all_data_cablam05x_session_data, responsiveness_df_cablam05x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standalone helper functions for handling removal of background ROIs and filtering responsive ROIs by stimulation ID\n",
    "\n",
    "def prepare_responsive_rois(session_data, responsiveness_df):\n",
    "    \"\"\"\n",
    "    Identifies valid responsive ROIs for each session, ensuring that only the ROIs still present in the session data\n",
    "    after background correction are included.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    session_data : dict\n",
    "        A dictionary where keys are session IDs and values are dataframes containing the calcium signals for each session.\n",
    "        The dataframes should include ROI columns (e.g., 'ROI_1', 'ROI_2', etc.) and any other metadata.\n",
    "    responsiveness_df : pd.DataFrame\n",
    "        A dataframe containing information about responsive ROIs. It must include columns such as 'session_id', 'roi',\n",
    "        'is_responsive', and 'stimulation_id'. This dataframe is used to identify which ROIs are considered responsive\n",
    "        based on stimulation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a session ID and the value is a list of valid responsive ROIs for that session.\n",
    "        The valid ROIs are those that are marked as responsive and still exist in the corresponding session data after\n",
    "        correction.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function filters ROIs based on their responsiveness to a specific stimulation (e.g., stimulation_id == 12).\n",
    "    - Only ROIs that are marked as responsive and still exist in the session dataframe after background correction\n",
    "      will be returned.\n",
    "    - This method is designed to handle the situation where some ROIs have been permanently removed due to background\n",
    "      correction in previous steps, and ensures downstream processes only use valid ROIs.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    valid_responsive_rois = prepare_responsive_rois(all_data_gcamp8_session_data, responsiveness_df_gcamp8)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold only valid responsive ROIs\n",
    "    valid_responsive_rois_by_session = {}\n",
    "\n",
    "    # Filter for responsive ROIs with stimulation_id == 12\n",
    "    responsive_df = responsiveness_df[\n",
    "        (responsiveness_df['is_responsive']) & \n",
    "        (responsiveness_df['stimulation_id'] == 12)\n",
    "    ]\n",
    "    \n",
    "    # Group by session_id to process each session separately\n",
    "    grouped_responsive_df = responsive_df.groupby('session_id')\n",
    "    \n",
    "    for session_id, group in grouped_responsive_df:\n",
    "        # Access the session's dataframe\n",
    "        session_df = session_data.get(session_id)\n",
    "        if session_df is None:\n",
    "            print(f\"Session ID {session_id} not found in session_data.\")\n",
    "            continue\n",
    "        \n",
    "        # Get the unique responsive ROIs\n",
    "        unique_rois = group['roi'].unique()\n",
    "        \n",
    "        # DEBUG: Print the ROIs in responsiveness_df\n",
    "        print(f\"Session {session_id}: Responsive ROIs in responsiveness_df: {unique_rois}\")\n",
    "        \n",
    "        # DEBUG: Print the available ROI columns in session_df\n",
    "        print(f\"Session {session_id}: Available ROI columns in session_df: {session_df.columns}\")\n",
    "        \n",
    "        # Check if the ROIs exist in the session dataframe columns\n",
    "        valid_rois = [roi for roi in unique_rois if roi in session_df.columns]\n",
    "        \n",
    "        if not valid_rois:\n",
    "            print(f\"No valid responsive ROIs found in session {session_id}.\")\n",
    "            continue\n",
    "        \n",
    "        # Store the valid ROIs in a dictionary\n",
    "        valid_responsive_rois_by_session[session_id] = valid_rois\n",
    "    \n",
    "    return valid_responsive_rois_by_session\n",
    "\n",
    "# Example use:\n",
    "valid_responsive_rois = prepare_responsive_rois(all_data_gcamp8_session_data, responsiveness_df_gcamp8)\n",
    "valid_responsive_rois\n",
    "\n",
    "def filter_responsive_rois_by_stimulation_correction(session_data, responsiveness_df, valid_responsive_rois_by_session):\n",
    "    \"\"\"\n",
    "    Filters session data for responsive ROIs corrected by background removal and ensures only valid ROIs are processed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    session_data : dict\n",
    "        Dictionary where keys are session IDs and values are dataframes containing the calcium signals for each session.\n",
    "    responsiveness_df : pd.DataFrame\n",
    "        Dataframe containing information about responsive ROIs, with columns such as 'session_id', 'roi', 'is_responsive', \n",
    "        and 'stimulation_id'.\n",
    "    valid_responsive_rois_by_session : dict\n",
    "        Dictionary where keys are session IDs and values are lists of valid ROIs that still exist after correction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a session ID and the value is a dataframe with the filtered responsive ROIs.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold the filtered dataframes\n",
    "    filtered_data_by_session = {}\n",
    "    \n",
    "    # Filter for responsive ROIs with stimulation_id == 12\n",
    "    responsive_df = responsiveness_df[\n",
    "        (responsiveness_df['is_responsive']) & \n",
    "        (responsiveness_df['stimulation_id'] == 12)\n",
    "    ]\n",
    "    \n",
    "    # Group by session_id to process each session separately\n",
    "    grouped_responsive_df = responsive_df.groupby('session_id')\n",
    "    \n",
    "    for session_id, group in grouped_responsive_df:\n",
    "        # Initialize a list to collect dataframes for this session\n",
    "        session_frames_list = []\n",
    "        \n",
    "        # Get unique ROIs for this session that are responsive\n",
    "        unique_rois = group['roi'].unique()\n",
    "        \n",
    "        # Access the session's dataframe\n",
    "        session_df = session_data.get(session_id)\n",
    "        if session_df is None:\n",
    "            print(f\"Session ID {session_id} not found in session_data.\")\n",
    "            continue\n",
    "        \n",
    "        # Get the valid ROIs from the valid_responsive_rois_by_session dictionary\n",
    "        valid_rois = valid_responsive_rois_by_session.get(session_id, [])\n",
    "        \n",
    "        # Filter the session dataframe for responsive ROIs that are still valid\n",
    "        for roi in unique_rois:\n",
    "            if roi in valid_rois:\n",
    "                # Extract the ROI number and construct the column name\n",
    "                roi_number = re.search(r'\\d+', roi)\n",
    "                if not roi_number:\n",
    "                    print(f\"ROI format is incorrect for {roi}\")\n",
    "                    continue\n",
    "                roi_column_name = f'ROI_{roi_number.group()}'\n",
    "                \n",
    "                if roi_column_name in session_df.columns:\n",
    "                    # Access the entire column for the responsive ROI\n",
    "                    roi_frames_df = session_df[[roi_column_name]].copy()\n",
    "                    \n",
    "                    # Add the ROI frames to the list for this session\n",
    "                    session_frames_list.append(roi_frames_df)\n",
    "                else:\n",
    "                    print(f\"Column {roi_column_name} not found in session dataframe for session_id {session_id}.\")\n",
    "            \n",
    "        # Combine the frames for the session into a single dataframe\n",
    "        if session_frames_list:\n",
    "            combined_frames_df = pd.concat(session_frames_list, axis=1)\n",
    "            # Store the filtered data in the dictionary using the session_id as the key\n",
    "            filtered_data_by_session[session_id] = combined_frames_df\n",
    "\n",
    "    return filtered_data_by_session\n",
    "# Call the new function\n",
    "gcamp8_filtered_responsive_rois = filter_responsive_rois_by_stimulation_correction(all_data_gcamp8_session_data, responsiveness_df_gcamp8, valid_responsive_rois)\n",
    "gcamp8_filtered_responsive_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###repeat for the cablam05x data \n",
    "\n",
    "#prepare the responsive rois for the cablam05x data\n",
    "valid_responsive_rois_cablam05x = prepare_responsive_rois(all_data_cablam05x_session_data, responsiveness_df_cablam05x)\n",
    "#call the filter_responsive_rois_by_stimulation_correction function for the cablam05x data\n",
    "cablam_filtered_responsive_rois05x = filter_responsive_rois_by_stimulation_correction(all_data_cablam05x_session_data, responsiveness_df_cablam05x, valid_responsive_rois_cablam05x)\n",
    "\n",
    "## repeat for the cablam1x data\n",
    "valid_responsive_rois_cablam1x = prepare_responsive_rois(all_data_cablam1x_session_data, responsiveness_df_cablam1x)\n",
    "cablam_filtered_responsive_rois = filter_responsive_rois_by_stimulation_correction(all_data_cablam1x_session_data, responsiveness_df_cablam1x, valid_responsive_rois_cablam1x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point, you can now use flexible plotting functions/methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the time series data for  ROIs for each session\n",
    "analysis_gcamp8.plot_session_time_series(gcamp8_filtered_responsive_rois)\n",
    "analysis_cablam1x.plot_session_time_series(cablam_filtered_responsive_rois)\n",
    "analysis_cablam05x.plot_session_time_series(cablam_filtered_responsive_rois05x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the method with specific ROIs for session 1212092023\n",
    "analysis_gcamp8.plot_session_time_series_for_specific_rois(\n",
    "    gcamp8_filtered_responsive_rois, \n",
    "    ['ROI_4', 'ROI_14', 'ROI_20'],  # List of ROIs to include in the plot, \n",
    "    frame_range=(0, 5000)  # Only plot frames from 1000 to 2000 (this will be converted to seconds)\n",
    ")\n",
    "\n",
    "# Run the method with specific ROIs for session 1212092023\n",
    "analysis_gcamp8.plot_session_time_series_for_specific_rois(\n",
    "    gcamp8_filtered_responsive_rois, \n",
    "    ['ROI_4', 'ROI_14', 'ROI_20'],  # List of ROIs to include in the plot, \n",
    "    frame_range=(3478, 5000)  # Only plot frames from 1000 to 2000 (this will be converted to seconds)\n",
    ")\n",
    "\n",
    "# Run the method with specific ROIs for session 1212092023\n",
    "analysis_gcamp8.plot_session_time_series_for_specific_rois(\n",
    "    gcamp8_filtered_responsive_rois, \n",
    "    ['ROI_4', 'ROI_14', 'ROI_20'],  # List of ROIs to include in the plot, \n",
    "    frame_range=(3409, 5000)  # Only plot frames from 1000 to 2000 (this will be converted to seconds)\n",
    ")\n",
    "\n",
    "\n",
    "analysis_cablam1x.plot_session_time_series_for_specific_rois(\n",
    "    cablam_filtered_responsive_rois, \n",
    "    ['ROI_18', 'ROI_3', 'ROI_14'], \n",
    "    frame_range=(0, 6326),\n",
    ")\n",
    "\n",
    "\n",
    "analysis_cablam1x.plot_session_time_series_for_specific_rois(\n",
    "    cablam_filtered_responsive_rois, \n",
    "    ['ROI_18', 'ROI_3', 'ROI_14'], \n",
    "    frame_range=(3409, 6326),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the method with specific ROIs for session 1212092023\n",
    "analysis_gcamp8.plot_session_time_series_for_specific_rois(\n",
    "    gcamp8_filtered_responsive_rois, \n",
    "    ['ROI_4', 'ROI_14', 'ROI_20'],  # List of ROIs to include in the plot, \n",
    "    frame_range=(0, 5000)  # Only plot frames from 1000 to 2000 (this will be converted to seconds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the time locked responses for the gcamp8, cablam1x, and cablam05x data ###\n",
    "analysis_gcamp8.plot_stim_responsiveness(\n",
    "    df=responsiveness_df_gcamp8,\n",
    "    include='responsive',\n",
    "    y_lim=None,\n",
    "    x_lim=(-10, 100),\n",
    "    mean_color='black',\n",
    "    figsize=(20, 6)\n",
    ")\n",
    "\n",
    "analysis_cablam05x.plot_stim_responsiveness(\n",
    "    df=responsiveness_df_cablam05x,\n",
    "    include='responsive',\n",
    "    y_lim=None,\n",
    "    x_lim=(-10, 100),\n",
    "    mean_color='blue',\n",
    "    figsize=(20, 6)\n",
    ")\n",
    "\n",
    "analysis_cablam1x.plot_stim_responsiveness(\n",
    "    df=responsiveness_df_cablam1x,\n",
    "    include='responsive',\n",
    "    y_lim=None,\n",
    "    x_lim=(-10, 100),\n",
    "    mean_color='red',\n",
    "    figsize=(20, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check how many ROIs are being plotted for each session\n",
    "def count_and_list_rois_per_stim(df, stim_ids=None, include='both'):\n",
    "    \"\"\"\n",
    "    Counts and lists the ROIs being plotted for each stimulation condition, optionally filtering by responsiveness.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the responsiveness data.\n",
    "    stim_ids : list, optional\n",
    "        List of stimulation IDs to filter on. If None, all unique IDs in the DataFrame will be used.\n",
    "    include : str, optional\n",
    "        Filter for 'responsive', 'non-responsive', or 'both' units.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a stimulation ID and the value is a dictionary containing:\n",
    "            - 'num_rois': The number of ROIs plotted for that stimulation.\n",
    "            - 'rois': A list of the ROIs being plotted for that stimulation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If stim_ids is not provided, get the unique IDs from the DataFrame and sort them\n",
    "    if stim_ids is None:\n",
    "        stim_ids = sorted(df['stimulation_id'].unique())\n",
    "    else:\n",
    "        stim_ids = sorted(stim_ids)\n",
    "    \n",
    "    # Dictionary to hold the ROI counts and lists per stimulation\n",
    "    rois_per_stim = {}\n",
    "\n",
    "    for stim_id in stim_ids:\n",
    "        # Filter the DataFrame based on the current stim_id\n",
    "        stim_df = df[df['stimulation_id'] == stim_id]\n",
    "\n",
    "        # Filter based on responsiveness if required\n",
    "        if include != 'both':\n",
    "            stim_df = stim_df[stim_df['is_responsive'] == (include == 'responsive')]\n",
    "        \n",
    "        # Get the unique ROIs for this stim_id\n",
    "        unique_rois = stim_df['roi'].unique()\n",
    "        \n",
    "        # Store the number of ROIs and the actual ROIs for this stimulation ID\n",
    "        rois_per_stim[stim_id] = {\n",
    "            'num_rois': len(unique_rois),\n",
    "            'rois': unique_rois.tolist()  # Convert the array to a list for easier readability\n",
    "        }\n",
    "    \n",
    "    return rois_per_stim\n",
    "\n",
    "# Assuming you have your data in a DataFrame called df\n",
    "rois_per_stim = count_and_list_rois_per_stim(responsiveness_df_cablam1x, include='responsive')\n",
    "\n",
    "# To view the output\n",
    "for stim_id, roi_info in rois_per_stim.items():\n",
    "    print(f\"Stimulation ID: {stim_id}\")\n",
    "    print(f\"Number of ROIs: {roi_info['num_rois']}\")\n",
    "    print(f\"ROIs: {roi_info['rois']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data in a DataFrame called d\n",
    "rois_per_stim = count_and_list_rois_per_stim(responsiveness_df_gcamp8, include='responsive')\n",
    "\n",
    "# To view the output\n",
    "for stim_id, roi_info in rois_per_stim.items():\n",
    "    print(f\"Stimulation ID: {stim_id}\")\n",
    "    print(f\"Number of ROIs: {roi_info['num_rois']}\")\n",
    "    print(f\"ROIs: {roi_info['rois']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SensorDataPlotter class methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare GCaMP8 and Cablam1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor names\n",
    "sensor_names_cablamvsgcamp = ['CaBLAM1x', 'GCaMP8s']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors2 = {\n",
    "    'CaBLAM1x': '#0000ff',   # Dark blue\n",
    "    'GCaMP8s': '#d3d3d3'   # Light grey\n",
    "}\n",
    "\n",
    "sensor_strip_colors2 = {\n",
    "    'CaBLAM1x': '#0000ff',   # Dark blue\n",
    "    'GCaMP8s': '#808080'   # Dark grey\n",
    "}\n",
    "# Initialize the SensorDataPlotter object\n",
    "cablamvsgcamp_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_cablam1x, responsiveness_df_gcamp8],\n",
    "    sensor_names=sensor_names_cablamvsgcamp,\n",
    "    sensor_box_colors=sensor_box_colors2,\n",
    "    sensor_strip_colors=sensor_strip_colors2\n",
    ") \n",
    "\n",
    "boxplot_directory_path = '/Volumes/MannySSD/cablam_imaging/cablam_final_figs/box_plots'\n",
    "cumulative_directory_path = '/Volumes/MannySSD/cablam_imaging/cablam_final_figs/cumlativedistribution'\n",
    "mean_error_lineplots_directory_path = '/Volumes/MannySSD/cablam_imaging/cablam_final_figs/mean_error_deltaf'\n",
    "timeseries_directory_path = '/Volumes/MannySSD/cablam_imaging/cablam_final_figs/time_series_allrois'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablamvsgcamp_plotter.plot_data('peak_delta_f_f_post_stim', \n",
    "                                selected_stim_ids=[12, 60, 480],\n",
    "                                strip_size=5, \n",
    "                                save_dir=boxplot_directory_path, \n",
    "                                save_dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablamvsgcamp_plotter.plot_cumulative_distribution(\n",
    "    column_name='time_to_peak',\n",
    "    selected_stim_ids=[12, 60, 480],\n",
    "    save_dir=cumulative_directory_path,\n",
    "    save_dpi=300, \n",
    "    num_points=40 # Adjust this value to change the step size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run stats test for the cablam vs gcamp8 data on the time to peak data\n",
    "ks_results = cablamvsgcamp_plotter.run_ks_test(\n",
    "    column_name='time_to_peak',\n",
    "    selected_stim_ids=[12, 60, 480]\n",
    ")\n",
    "\n",
    "ks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablamvsgcamp_plotter.plot_time_series(\n",
    "    'delta_f_f_full_array',\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    fig_size=(6.5, 8),  # Figure size\n",
    "    dpi=300,  # Display resolution\n",
    "    y_limits=None,  # Set custom y-axis limits (min, max)\n",
    "    save_dir=timeseries_directory_path,  # Directory where the plot will be saved\n",
    "    save_dpi=300,  # Resolution for saving the figure\n",
    "    plot_sem=True,  # Enable plotting of the SEM, \n",
    "    plot_sem_as_dotted=True)  # Plot the SEM as dotted lines instead of a shaded region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### repeat the comparison for the cablam05x data and cablam1x data\n",
    "sensor_names_cablam05xvscablam1x = ['CaBLAM05x', 'CaBLAM1x']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors3 = {\n",
    "    'CaBLAM05x': '#9999ff',  # Light blue\n",
    "    'CaBLAM1x': '#0000ff'   # Dark blue\n",
    "}\n",
    "\n",
    "sensor_strip_colors3 = {\n",
    "    'CaBLAM05x': '#9999ff', # Light blue\n",
    "    'CaBLAM1x': '#0000ff'   # Dark blue\n",
    "}\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "cablam05xvscablam1x_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_cablam05x, responsiveness_df_cablam1x],\n",
    "    sensor_names=sensor_names_cablam05xvscablam1x,\n",
    "    sensor_box_colors=sensor_box_colors3,\n",
    "    sensor_strip_colors=sensor_strip_colors3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablam05xvscablam1x_plotter.plot_data('peak_delta_f_f_post_stim', \n",
    "                                selected_stim_ids=[12, 24, 120, 480, 1920],\n",
    "                                strip_size=5, \n",
    "                                save_dir=boxplot_directory_path, \n",
    "                                save_dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablam05xvscablam1x_plotter.plot_cumulative_distribution(\n",
    "    column_name='time_to_peak',\n",
    "    selected_stim_ids=[12, 24, 120, 480, 1920],\n",
    "    save_dir=cumulative_directory_path,\n",
    "    save_dpi=300, \n",
    "    num_points=40 # Adjust this value to change the step size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run stats test for the cablam vs gcamp8 data on the time to peak data\n",
    "ks_results_cablam05xvscablam1x_plotter = cablam05xvscablam1x_plotter.run_ks_test(\n",
    "    column_name='time_to_peak',\n",
    "    selected_stim_ids=[12, 24, 120, 480, 1920]\n",
    ")\n",
    "\n",
    "ks_results_cablam05xvscablam1x_plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for some reason, I haev to reinitlaize the plotter object to run the plot_mean_with_error function with different stim ids \n",
    "## need to fix this in the future\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "cablam05xvscablam1x_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_cablam05x, responsiveness_df_cablam1x],\n",
    "    sensor_names=sensor_names_cablam05xvscablam1x,\n",
    "    sensor_box_colors=sensor_box_colors3,\n",
    "    sensor_strip_colors=sensor_strip_colors3\n",
    ")\n",
    "\n",
    "\n",
    "cablam05xvscablam1x_plotter.plot_mean_with_error(\n",
    "    'peak_delta_f_f_post_stim',\n",
    "    error_type='SEM',\n",
    "    selected_stim_ids=[12, 24, 36, 60, 120, 240, 480, 1920],\n",
    "    save_dir=mean_error_lineplots_directory_path,\n",
    "    save_dpi=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablam05xvscablam1x_plotter.plot_time_series(\n",
    "    'delta_f_f_full_array',\n",
    "    selected_stim_ids=[12, 24, 120, 480, 1920],  # List of stimulation IDs to plot\n",
    "    fig_size=(6.5, 8),  # Figure size\n",
    "    dpi=300,  # Display resolution\n",
    "    y_limits=None,  # Set custom y-axis limits (min, max)\n",
    "    save_dir=timeseries_directory_path,  # Directory where the plot will be saved\n",
    "    save_dpi=300,  # Resolution for saving the figure\n",
    "    plot_sem=True,  # Enable plotting of the SEM, \n",
    "    plot_sem_as_dotted=True  # Plot the SEM as dotted lines instead of a shaded region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze a single sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor names\n",
    "sensor_names_gcamp = ['GCaMP8s']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors_gcamp = {\n",
    "    'GCaMP8s': '#d3d3d3'   # Light grey\n",
    "}\n",
    "\n",
    "sensor_strip_colors_gcamp = {\n",
    "    'GCaMP8s': '#808080'   # Dark grey\n",
    "}\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "gcamp_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_gcamp8],\n",
    "    sensor_names=sensor_names_gcamp,\n",
    "    sensor_box_colors=sensor_box_colors_gcamp,\n",
    "    sensor_strip_colors=sensor_strip_colors_gcamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcamp_plotter.plot_mean_with_error(\n",
    "    'peak_delta_f_f_post_stim',\n",
    "    error_type='SEM',\n",
    "    selected_stim_ids=[12, 24, 120, 480],\n",
    "    save_dir=mean_error_lineplots_directory_path,\n",
    "    save_dpi=300\n",
    ")\n",
    "\n",
    "\n",
    "gcamp_plotter.plot_time_series(\n",
    "    'delta_f_f_full_array',\n",
    "    selected_stim_ids=[12, 24, 120, 480],  # List of stimulation IDs to plot\n",
    "    fig_size=(6.5, 8),  # Figure size\n",
    "    dpi=300,  # Display resolution\n",
    "    y_limits=None,  # Set custom y-axis limits (min, max)\n",
    "    save_dir=timeseries_directory_path,  # Directory where the plot will be saved\n",
    "    save_dpi=300,  # Resolution for saving the figure\n",
    "    plot_sem=True,  # Enable plotting of the SEM, \n",
    "    plot_sem_as_dotted=True  # Plot the SEM as dotted lines instead of a shaded region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmaps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### must re runn this as a group, need to update the udnerlyuin gmethod to handle this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repeat the same process for the cablam data\n",
    "# Sensor names\n",
    "sensor_names_cablam = ['CaBLAM1x']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors_cablam = {\n",
    "    'CaBLAM1x': '#ccccff'   # Light blue\n",
    "}\n",
    "\n",
    "sensor_strip_colors_cablam = {\n",
    "    'CaBLAM1x': '#0000ff'   # Dark blue\n",
    "}\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "cablam_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_cablam1x],\n",
    "    sensor_names=sensor_names_cablam,\n",
    "    sensor_box_colors=sensor_box_colors_cablam,\n",
    "    sensor_strip_colors=sensor_strip_colors_cablam\n",
    ")\n",
    "\n",
    "cablam_plotter.plot_non_responsive_heatmap_and_pie(selected_stim_id=480, \n",
    "                                                   vmin=0, \n",
    "                                                   vmax=8,\n",
    "                                                   smooth_method=None, \n",
    "                                                   smooth_sigma=1, \n",
    "                                                   save_dir='/Volumes/MannySSD/cablam_imaging/cablam_final_figs/heatmaps', \n",
    "                                                   save_dpi=300, \n",
    "                                                   interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeatr for the cablam05x data\n",
    "# Sensor names\n",
    "sensor_names_cablam05x = ['CaBLAM05x']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors_cablam05x = {\n",
    "    'CaBLAM05x': '#9999ff'   # Light blue\n",
    "}\n",
    "\n",
    "sensor_strip_colors_cablam05x = {\n",
    "    'CaBLAM05x': '#9999ff'   # Light blue\n",
    "}\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "cablam05x_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_cablam05x],\n",
    "    sensor_names=sensor_names_cablam05x,\n",
    "    sensor_box_colors=sensor_box_colors_cablam05x,\n",
    "    sensor_strip_colors=sensor_strip_colors_cablam05x\n",
    ")\n",
    "\n",
    "cablam05x_plotter.plot_non_responsive_heatmap_and_pie(selected_stim_id=480,\n",
    "                                                        vmin=0,\n",
    "                                                        vmax=8,\n",
    "                                                        smooth_method=None,\n",
    "                                                        smooth_sigma=1,\n",
    "                                                        save_dir='/Volumes/MannySSD/cablam_imaging/cablam_final_figs/heatmaps',\n",
    "                                                        save_dpi=300,\n",
    "                                                        interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for the gcamp8 data\n",
    "# Sensor names\n",
    "sensor_names_gcamp = ['GCaMP8s']\n",
    "\n",
    "# Dictionaries for sensor colors (boxplot and stripplot)\n",
    "sensor_box_colors_gcamp = {\n",
    "    'GCaMP8s': '#d3d3d3'   # Light grey\n",
    "}\n",
    "\n",
    "sensor_strip_colors_gcamp = {\n",
    "    'GCaMP8s': '#808080'   # Dark grey\n",
    "}\n",
    "\n",
    "# Initialize the SensorDataPlotter object\n",
    "gcamp_plotter = SensorDataPlotter(\n",
    "    data_frames=[responsiveness_df_gcamp8],\n",
    "    sensor_names=sensor_names_gcamp,\n",
    "    sensor_box_colors=sensor_box_colors_gcamp,\n",
    "    sensor_strip_colors=sensor_strip_colors_gcamp\n",
    ")\n",
    "\n",
    "\n",
    "#repeat for gcamp8 data\n",
    "gcamp_plotter.plot_non_responsive_heatmap_and_pie(selected_stim_id=480, \n",
    "                                                  vmin=0, \n",
    "                                                  vmax=None,\n",
    "                                                  smooth_method=None, \n",
    "                                                  smooth_sigma=1, \n",
    "                                                  save_dir='/Volumes/MannySSD/cablam_imaging/gcamp8_final_figs/heatmaps', \n",
    "                                                  save_dpi=300, \n",
    "                                                  interpolation='nearest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot each sensor type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the figures\n",
    "save_directory = '/Volumes/MannySSD/cablam_imaging/cablam_final_figs/Timeseries_evoked'\n",
    "\n",
    "gcamp_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='1212092023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_4',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.5, 1.0)  # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")\n",
    "\n",
    "gcamp_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='1212092023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_14',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.5, 1.0)   # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")\n",
    "\n",
    "gcamp_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='1212092023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_20',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.5, 1.0)   # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cablam_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='2112232023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_18',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.9, 7)   # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")\n",
    "\n",
    "\n",
    "cablam_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='2112232023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_3',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.9, 7)   # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")\n",
    "\n",
    "\n",
    "cablam_plotter.plot_single_roi(\n",
    "    full_array_column='delta_f_f_full_array',  # Replace with the actual column name for time series data\n",
    "    session_id='2112232023',  # Replace with the session ID you want to plot\n",
    "    roi='ROI_14',  # Replace with the ROI you want to plot\n",
    "    selected_stim_ids=[12, 60, 480],  # List of stimulation IDs to plot\n",
    "    save_dir=save_directory,  # Replace with the directory where you want to save the plot (or None to skip saving)\n",
    "    y_limits=(-0.9, 7)  # Optional: Replace with your desired y-axis limits or leave as None\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biolumi_calcium_imaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
